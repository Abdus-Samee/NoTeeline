{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai tiktoken chromadb langchain"
      ],
      "metadata": {
        "id": "gZwTgdY5Zz8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/modules/chains/document/map_reduce"
      ],
      "metadata": {
        "id": "jgHnB8869l9U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a token: https://platform.openai.com/account/api-keys\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6auwr9QA6IQ0",
        "outputId": "7c54024d-cf08-446c-8f22-4a5f3c4224c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader('https://en.wikipedia.org/wiki/Ethos') #(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-1106-preview\")"
      ],
      "metadata": {
        "id": "PXkilMa0Z2XO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing splitter if needed for chunking a document into smaller parts\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "# text_splitter = CharacterTextSplitter(\n",
        "#     separator = \"\\n\\n\",\n",
        "#     chunk_size = 1000,\n",
        "#     chunk_overlap  = 200,\n",
        "#     length_function = len,\n",
        "#     is_separator_regex = False,\n",
        "# )\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 100,\n",
        "    separators=[\".\", \",\", \"\\n\"]\n",
        ")"
      ],
      "metadata": {
        "id": "-LzAFS3ngqF3"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.prompts import format_document\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# Prompt and method for converting Document -> str.\n",
        "document_prompt = PromptTemplate.from_template(\"{page_content}\")\n",
        "partial_format_document = partial(format_document, prompt=document_prompt)\n",
        "\n",
        "# The chain we'll apply to each individual document.\n",
        "# Returns a summary of the document.\n",
        "map_chain = (\n",
        "    {\"context\": partial_format_document}\n",
        "    | PromptTemplate.from_template(\"\"\"Provide a summary of the following text.\n",
        "                  TEXT: {context}\n",
        "                  SUMMARY:\"\"\") #(\"Summarize this content:\\n\\n{context}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# A wrapper chain to keep the original Document metadata\n",
        "map_as_doc_chain = (\n",
        "    RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
        "    | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata)) # Document(page_content=x, metadata=x)\n",
        ").with_config(run_name=\"Summarize (return doc)\")\n",
        "\n",
        "# The chain we'll repeatedly apply to collapse subsets of the documents\n",
        "# into a consolidate document until the total token size of our\n",
        "# documents is below some max size.\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
        "\n",
        "\n",
        "collapse_chain = (\n",
        "    {\"context\": format_docs}\n",
        "    | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "def get_num_tokens(docs):\n",
        "    return llm.get_num_tokens(format_docs(docs))\n",
        "\n",
        "\n",
        "def collapse(\n",
        "    docs,\n",
        "    config,\n",
        "    token_max=8000,\n",
        "):\n",
        "    collapse_ct = 1\n",
        "    while get_num_tokens(docs) > token_max:\n",
        "        config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
        "        invoke = partial(collapse_chain.invoke, config=config)\n",
        "        split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
        "        docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
        "        collapse_ct += 1\n",
        "    return docs\n",
        "\n",
        "# The chain we'll use to combine our individual document summaries\n",
        "# (or summaries over subset of documents if we had to collapse the map results)\n",
        "# into a final summary.\n",
        "\n",
        "reduce_chain = (\n",
        "    {\"context\": format_docs}\n",
        "    | PromptTemplate.from_template('''Write a concise summary of the following text delimited by triple backquotes. Focus on the main content of the paragraph, not on the style or the structure.\n",
        "                      ```{context}```\n",
        "                      SUMMARY:''') #(\"Combine these summaries:\\n\\n{context}\")\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ").with_config(run_name=\"Reduce\")\n",
        "\n",
        "# # The final full chain as shown in the documentation\n",
        "# map_reduce_with_collapse = (map_as_doc_chain.map() | collapse | reduce_chain).with_config(\n",
        "#     run_name=\"Map reduce\"\n",
        "# )\n",
        "\n",
        "# The final full chain without collapse, I do not want to collapse\n",
        "map_reduce = (map_as_doc_chain.map() | reduce_chain).with_config(\n",
        "    run_name=\"Map reduce\"\n",
        ")"
      ],
      "metadata": {
        "id": "MfgIvrGPhn4m"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "loader = WebBaseLoader('https://en.wikipedia.org/wiki/Ethos') #(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "3-tU9HxFg79J"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = text_splitter.split_text(docs[0].page_content)\n",
        "new_docs = text_splitter.create_documents(splits) # https://stackoverflow.com/a/77464710\n",
        "# https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb\n",
        "len(new_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOPHo7si9gbR",
        "outputId": "35db5c3c-842a-45e9-d70d-aa4ff1079278"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(map_reduce.invoke(docs[0:1], config={\"max_concurrency\": 5}))\n",
        "\n",
        "print(map_reduce.invoke(new_docs, config={\"max_concurrency\": 5}))"
      ],
      "metadata": {
        "id": "WbhPt_h230gC",
        "outputId": "770ad23e-7051-43ac-e3cf-b745c5d161f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text is a detailed description of a Wikipedia page about \"Ethos,\" a Greek term meaning 'character' that refers to the guiding beliefs or ideals of a community, nation, or ideology. The page includes navigation options, sections for contributing to Wikipedia, and language links. The content covers the etymology and origin of \"ethos,\" its use in rhetoric as one of Aristotle's three modes of persuasion, and its significance in various contexts such as music, politics, and public life. The page also discusses the portrayal of character in Greek tragedy, the concept of ethos in visual arts, and modern interpretations of ethos in rhetoric, including feminist perspectives and the influence of cultural values. Additionally, the text lists notable rhetoricians, influential works on rhetoric, and references to academic works on related topics. The page may require structural improvements and has metadata, categories, and maintenance tags indicating its status and content licensing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YwSLHKKmMc-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}