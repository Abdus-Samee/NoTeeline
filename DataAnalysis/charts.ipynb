{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083867b-4edb-40b1-9ecb-a3ac860e3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: Dictionary where keys are person IDs and values are lists of time intervals\n",
    "data = {\n",
    "    1: [(10, 20), (30, 50), (60, 80)],  # For person ID 1, time intervals are (10, 20), (30, 50), and (60, 80)\n",
    "    2: [(15, 25), (40, 60), (70, 90)],  # For person ID 2, time intervals are (15, 25), (40, 60), and (70, 90)\n",
    "    3: [(5, 15), (35, 55), (65, 85)]    # For person ID 3, time intervals are (5, 15), (35, 55), and (65, 85)\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "\n",
    "for person_id, intervals in data.items():\n",
    "    for interval in intervals:\n",
    "        start, end = interval\n",
    "        plt.barh(person_id, width=end-start, left=start, height=0.1, color='#4338CA')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Person ID')\n",
    "plt.title('Time Intervals for Person IDs')\n",
    "plt.yticks(list(data.keys()), ['Person {}'.format(pid) for pid in data.keys()])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076300d-6c32-4301-9305-258de40e7746",
   "metadata": {},
   "source": [
    "# Create dictionary of user logs on video logistics and note logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c714254c-9159-4d45-850c-6877cba720b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "data = []\n",
    "# pd = {}\n",
    "vd = {}\n",
    "auto_note = []\n",
    "manual_note = []\n",
    "\n",
    "# for user_folder in os.listdir(userlog_path):\n",
    "for folder_number in range(1, 13):\n",
    "    user_folder = f'P{folder_number}'\n",
    "    print(f'Processing {user_folder}')\n",
    "    user_data = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        # if folder_number % 2 == 0:\n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "\n",
    "                str_rep = ''\n",
    "                \n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    pass\n",
    "                # if not file.lower().startswith('video1') and not file.lower().startswith('video2'): continue\n",
    "                else:\n",
    "                    video_data = {}\n",
    "                    vd_data = {}\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    video_data['pauseCount'] = bullet_points_data['pauseCount']\n",
    "                    video_data['forwardCount'] = bullet_points_data['forwardCount']\n",
    "                    video_data['reverseCount'] = bullet_points_data['reverseCount']\n",
    "                    video_data['point_count'] = len(bullet_points_data['editHistory'])\n",
    "                    summary_t = bullet_points_data['summary_t']\n",
    "                    summary_p = bullet_points_data['summary_p']\n",
    "                    note_points = [\n",
    "                        {\n",
    "                            'point': bpd['point'], \n",
    "                            'time_taken': bpd['note_taking_time'],\n",
    "                            'timestamp': bpd['utc_time'],\n",
    "                            'expanded_note': bpd['edit'][-1][0]['e_point'] if len(bpd['edit']) > 1 else None,\n",
    "                            'transcript': bpd['fraction_transcript'],\n",
    "                            'v_id': file.lower()\n",
    "                        }\n",
    "                        for bpd in bullet_points_data['editHistory']\n",
    "                    ]\n",
    "\n",
    "                    for bpd in bullet_points_data['editHistory']:\n",
    "                        str_rep += bpd['edit'][-1][0]['e_point']\n",
    "\n",
    "                    vd_data['p_id'] = user_folder\n",
    "                    vd_data['note_points'] = note_points\n",
    "                    vd_data['summary_p'] = summary_p\n",
    "                    vd_data['summary_t'] = summary_t\n",
    "                    if folder_number % 2 == 0:\n",
    "                        if file.lower().startswith('video1'): \n",
    "                            user_data['Baseline'] = video_data\n",
    "                            vd_data['micronote'] = False\n",
    "                            if 'video1' not in vd:\n",
    "                                vd['video1'] = []\n",
    "                            vd['video1'].append(vd_data)\n",
    "                            manual_note.append(str_rep)\n",
    "                        elif file.lower().startswith('video2'): \n",
    "                            user_data['NoTeeline'] = video_data\n",
    "                            vd_data['micronote'] = True\n",
    "                            if 'video2' not in vd:\n",
    "                                vd['video2'] = []\n",
    "                            vd['video2'].append(vd_data)\n",
    "                            auto_note.append(str_rep)\n",
    "                    else:\n",
    "                        if file.lower().startswith('video1'): \n",
    "                            user_data['NoTeeline'] = video_data\n",
    "                            vd_data['micronote'] = True\n",
    "                            if 'video1' not in vd:\n",
    "                                vd['video1'] = []\n",
    "                            vd['video1'].append(vd_data)\n",
    "                            auto_note.append(str_rep)\n",
    "                        elif file.lower().startswith('video2'): \n",
    "                            user_data['Baseline'] = video_data\n",
    "                            vd_data['micronote'] = False\n",
    "                            if 'video2' not in vd:\n",
    "                                vd['video2'] = []\n",
    "                            vd['video2'].append(vd_data)\n",
    "                            manual_note.append(str_rep)\n",
    "    data.append(user_data)\n",
    "\n",
    "# print('############ COUNT DATA ############')\n",
    "# for i, d in enumerate(data):\n",
    "#     print('----------------------------------------')\n",
    "#     print(f'User {i+1}')\n",
    "#     print(d)\n",
    "\n",
    "# print('\\n############ VIDEO DATA ############')\n",
    "# for video in vd:\n",
    "#     print(f'--------------\\n{video} data...\\n--------------')\n",
    "#     for v in vd[video]:\n",
    "#         print('p_id\\n----')\n",
    "#         print(f\"{v['p_id']}\")\n",
    "#         print('micronote\\n---------')\n",
    "#         print(f\"{v['micronote']}\")\n",
    "#         print('summary_t\\n---------')\n",
    "#         print(f\"{v['summary_t']}\")\n",
    "#         print('summary_p\\n---------')\n",
    "#         print(f\"{v['summary_p']}\")\n",
    "#         print(f\"Notes:\\n------\\n{v['note_points']}\")\n",
    "#         print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7405a01-fbe4-4ebc-9dc5-37238afccd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "chosen_cmap = matplotlib.cm.get_cmap('Paired')\n",
    "# color_array = [chosen_cmap(i) for i in np.linspace(0, 1, 6)]\n",
    "# color_hex_array = [matplotlib.colors.to_hex(color) for color in color_array]\n",
    "\n",
    "# Lists to hold the plot data\n",
    "labels = []\n",
    "noteeline_pause = []\n",
    "noteeline_star_pause = []\n",
    "noteeline_forward = []\n",
    "noteeline_star_forward = []\n",
    "noteeline_reverse = []\n",
    "noteeline_star_reverse = []\n",
    "\n",
    "# Populate the lists with data\n",
    "for user, activities in enumerate(data):\n",
    "    # print(activities)\n",
    "    labels.append(f'P{user+1}')\n",
    "    noteeline_pause.append(activities['NoTeeline']['pauseCount'])\n",
    "    noteeline_star_pause.append(activities['Baseline']['pauseCount'])\n",
    "    noteeline_forward.append(activities['NoTeeline']['forwardCount'])\n",
    "    noteeline_star_forward.append(activities['Baseline']['forwardCount'])\n",
    "    noteeline_reverse.append(activities['NoTeeline']['reverseCount'])\n",
    "    noteeline_star_reverse.append(activities['Baseline']['reverseCount'])\n",
    "\n",
    "# Width of the bars\n",
    "barWidth = 0.3\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Make the plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Stacked bar for 'noteeline'\n",
    "# bar1 = plt.bar(r1, noteeline_pause, color=chosen_cmap(1), width=barWidth, edgecolor='white', label='Pause (NoTeeline)')\n",
    "# bar2 = plt.bar(r1, noteeline_forward, bottom=noteeline_pause, color=chosen_cmap(3), width=barWidth, edgecolor='white', label='Seek Forward (NoTeeline)')\n",
    "# bar3 = plt.bar(r1, noteeline_reverse, bottom=[i+j for i,j in zip(noteeline_pause, noteeline_forward)], color=chosen_cmap(5), width=barWidth, edgecolor='white', label='Seek Backward (NoTeeline)')\n",
    "\n",
    "# # Stacked bar for 'noteeline*'\n",
    "# bar4 =plt.bar(r2, noteeline_star_pause, color=chosen_cmap(0), width=barWidth, edgecolor='white', label='Pause (Baseline)')\n",
    "# bar5 = plt.bar(r2, noteeline_star_forward, bottom=noteeline_star_pause, color=chosen_cmap(2), width=barWidth, edgecolor='white', label='Seek Forward (Baseline)')\n",
    "# bar6 = plt.bar(r2, noteeline_star_reverse, bottom=[i+j for i,j in zip(noteeline_star_pause, noteeline_star_forward)], color=chosen_cmap(4), width=barWidth, edgecolor='white', label='Seek Backword (Baseline)')\n",
    "\n",
    "# # Add xticks on the middle of the group bars\n",
    "# # plt.xlabel('User', fontweight='bold')\n",
    "# plt.xticks([r + barWidth/2 for r in range(len(labels))], labels)\n",
    "# plt.legend(loc=\"lower left\", ncol=2, bbox_to_anchor=(0.18, -0.3), frameon=False) # https://stackoverflow.com/a/54870844\n",
    "# plt.yticks([])  \n",
    "\n",
    "# # Create legend & Show graphic\n",
    "# # plt.legend()\n",
    "# plt.box(False)\n",
    "# plt.savefig('video_log_dist.pdf', bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f354e-c89f-4756-aa72-c54b3668807d",
   "metadata": {},
   "source": [
    "# Save Results as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117676b-877e-4269-9dd5-5cd485e0e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the note_leng_log dictionary\n",
    "note_leng_log = {}\n",
    "\n",
    "# Loop over videos and their data\n",
    "for video in vd:\n",
    "    # print(f'--------------\\n{video} data...\\n--------------')\n",
    "    for v in vd[video]:\n",
    "        # print('p_id\\n----')\n",
    "        # print(f\"{v['p_id']}\")\n",
    "        # print('micronote\\n---------')\n",
    "        # print(f\"{v['micronote']}\")\n",
    "\n",
    "        if v['micronote']: \n",
    "            for item in v['note_points']:\n",
    "                # Prepare a dictionary for the current entry\n",
    "                current_entry = {\n",
    "                    'tool': 'NoTeeline',\n",
    "                    'micro_note': item['point'],\n",
    "                    'full_note': item['expanded_note'],\n",
    "                    'transcript': item['transcript'],\n",
    "                    'time_taken': item['time_taken'],\n",
    "                    'timestamp': item['timestamp'],\n",
    "                    'v_id': item['v_id']\n",
    "                }\n",
    "                # Append or create a dict item in note_leng_log for the current p_id\n",
    "                if v['p_id'] in note_leng_log:\n",
    "                    note_leng_log[v['p_id']].append(current_entry)\n",
    "                else:\n",
    "                    note_leng_log[v['p_id']] = [current_entry]\n",
    "        else:\n",
    "            for item in v['note_points']:\n",
    "                # Prepare a dictionary for the current entry\n",
    "                current_entry = {\n",
    "                    'tool': 'Baseline',\n",
    "                    'micro_note': item['point'],\n",
    "                    'full_note': '',\n",
    "                    'transcript': item['transcript']}\n",
    "\n",
    "                # Append or create a dict item in note_leng_log for the current p_id\n",
    "                if v['p_id'] in note_leng_log:\n",
    "                    note_leng_log[v['p_id']].append(current_entry)\n",
    "                else:\n",
    "                    note_leng_log[v['p_id']] = [current_entry]\n",
    "\n",
    "# 'note_leng_log' now contains the required log information\n",
    "\n",
    "# Flatten the note_leng_log dict to a list of dicts, ensuring p_id is the first key\n",
    "flat_data = []\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    for entry in entries:\n",
    "        # Construct a new dictionary with p_id as the first key\n",
    "        entry_with_pid = {'p_id': p_id, **entry}\n",
    "        flat_data.append(entry_with_pid)\n",
    "\n",
    "# Convert the flat_data list of dicts to a pandas DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# Now p_id will be the first column in the DataFrame\n",
    "# print(df.head())\n",
    "df.to_csv('all_points.csv', index=False) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9330e-a159-453e-aa26-e178d22952f2",
   "metadata": {},
   "source": [
    "# Summary Log Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fe9d2-e34b-46af-afb7-63201351e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "data = []\n",
    "total_ = 0\n",
    "wrong_ = 0\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key='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',\n",
    ")\n",
    "\n",
    "def generate_point_summary(points: str, context: str) -> str:\n",
    "    user_prompt = (\n",
    "        f'''I will give you a context and some keypoints, Your task is to summarize the keypoints in 4 sentences.\n",
    "        Focus on the keypoint, only use context if you need extra information:\n",
    "        Context: {context}\n",
    "        Keypoints: {points}\n",
    "        Remember not to make it too long.\n",
    "        Do not mark the sentences with 1,2 etc.'''\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",  # Make sure to use the correct model name\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=150,  # Adjust max_tokens if necessary\n",
    "        # seed=SEED,  # Optionally, set a seed for deterministic output, if needed\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# for user_folder in os.listdir(userlog_path):\n",
    "for folder_number in range(1, 13):\n",
    "    user_folder = f'P{folder_number}'\n",
    "    print(f'Processing {user_folder}')\n",
    "    user_data = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        \n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    continue\n",
    "                # if not file.lower().startswith('video1') and not file.lower().startswith('video2'): continue\n",
    "                else:\n",
    "                    vd_data = {}\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    \n",
    "                    note_points = ''\n",
    "                    for bpd in bullet_points_data['editHistory']:\n",
    "                        note_points += bpd['edit'][-1][0]['e_point']\n",
    "                        total_ += 1\n",
    "\n",
    "                        # checking error\n",
    "                        if len(bpd['edit']) > 1:\n",
    "                            expanded_note_string = bpd['edit'][-1][0]['e_point']\n",
    "                            if 'sorry' in expanded_note_string.lower():\n",
    "                                print(expanded_note_string)\n",
    "                                wrong_ += 1\n",
    "                            elif 'please' in expanded_note_string.lower():\n",
    "                                print(expanded_note_string)\n",
    "                                wrong_ += 1\n",
    "                            elif 'not found' in expanded_note_string.lower():\n",
    "                                print(expanded_note_string)\n",
    "                                wrong_ += 1\n",
    "            \n",
    "\n",
    "                    summary_t = bullet_points_data['summary_t']\n",
    "                    if bullet_points_data['summary_p'] == '':\n",
    "                        if bullet_points_data['summary_t'] != '':\n",
    "                            summary_p = generate_point_summary(note_points, summary_t)\n",
    "                            print(user_folder, file)\n",
    "                            print(summary_p)\n",
    "                    else:\n",
    "                        summary_p = bullet_points_data['summary_p']\n",
    "\n",
    "                        \n",
    "                    vd_data['p_id'] = user_folder\n",
    "                    vd_data['file'] = file\n",
    "                    vd_data['note_points'] = note_points\n",
    "                    vd_data['summary_p'] = summary_p\n",
    "                    vd_data['summary_t'] = summary_t\n",
    "\n",
    "                    data.append(vd_data)\n",
    "\n",
    "print(total_, wrong_)\n",
    "df = pd.DataFrame(data)\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9416a16-db54-4c43-9ae0-ac94d234b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the note_leng_log dictionary\n",
    "note_leng_log = {}\n",
    "\n",
    "# Loop over videos and their data\n",
    "for video in vd:\n",
    "    print(f'--------------\\n{video} data...\\n--------------')\n",
    "    for v in vd[video]:\n",
    "        print('p_id\\n----')\n",
    "        print(f\"{v['p_id']}\")\n",
    "        print('micronote\\n---------')\n",
    "        print(f\"{v['micronote']}\")\n",
    "\n",
    "        # Calculate statistics based on the provided 'data'\n",
    "        time_taken_values = [item['time_taken']/1000 for item in v['note_points']]\n",
    "        mean_time_taken = np.mean(time_taken_values)\n",
    "        std_time_taken = np.std(time_taken_values)\n",
    "        \n",
    "        point_lengths = [len(item['point']) for item in v['note_points']]\n",
    "        mean_point_length = np.mean(point_lengths)\n",
    "        std_point_length = np.std(point_lengths)\n",
    "\n",
    "        # Prepare a dictionary for the current entry\n",
    "        current_entry = {\n",
    "            'tool': 'NoTeeline' if v['micronote'] else 'Baseline',\n",
    "            'length': len(v['note_points']),\n",
    "            'mean_time_taken': mean_time_taken,\n",
    "            'std_time_taken': std_time_taken,\n",
    "            'mean_point_length': mean_point_length,\n",
    "            'std_point_length': std_point_length\n",
    "        }\n",
    "\n",
    "        # Append or create a dict item in note_leng_log for the current p_id\n",
    "        if v['p_id'] in note_leng_log:\n",
    "            note_leng_log[v['p_id']].append(current_entry)\n",
    "        else:\n",
    "            note_leng_log[v['p_id']] = [current_entry]\n",
    "\n",
    "# 'note_leng_log' now contains the required log information\n",
    "\n",
    "# Flatten the note_leng_log dict to a list of dicts, ensuring p_id is the first key\n",
    "flat_data = []\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    for entry in entries:\n",
    "        # Construct a new dictionary with p_id as the first key\n",
    "        entry_with_pid = {'p_id': p_id, **entry}\n",
    "        flat_data.append(entry_with_pid)\n",
    "\n",
    "# Convert the flat_data list of dicts to a pandas DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# Now p_id will be the first column in the DataFrame\n",
    "# print(df.head())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1979b-233e-4666-80d7-5a3b4edaa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Assuming chosen_cmap is defined somewhere else in your code, for example:\n",
    "# chosen_cmap = cm.get_cmap('YourColormapName')\n",
    "\n",
    "noteeline_cnt = []\n",
    "baseline_cnt = []\n",
    "noteeline_len = []\n",
    "baseline_len = []\n",
    "noteeline_time = []\n",
    "baseline_time = []\n",
    "pids = []\n",
    "x = []\n",
    "\n",
    "_iter = 1\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    pids.append(p_id)\n",
    "    x.append(_iter)\n",
    "    _iter += 1\n",
    "    for entry in entries:\n",
    "        if entry['tool'] == 'NoTeeline':\n",
    "            noteeline_cnt.append(entry['length'])\n",
    "            noteeline_len.append(entry['mean_point_length'])\n",
    "            noteeline_time.append(entry['mean_time_taken'])\n",
    "        else:\n",
    "            baseline_cnt.append(entry['length'])\n",
    "            baseline_len.append(entry['mean_point_length'])\n",
    "            baseline_time.append(entry['mean_time_taken'])\n",
    "\n",
    "# Define font sizes for different parts of the plot\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 26\n",
    "\n",
    "# Set font sizes\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Plot for count of notes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, noteeline_cnt, label='NoTeeline', marker='o', color=chosen_cmap(3))\n",
    "plt.plot(x, baseline_cnt, label='Baseline', marker='o', color=chosen_cmap(9))\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.35), ncol=2, frameon=False)\n",
    "plt.xticks([r+1 for r in range(len(pids))], pids)\n",
    "plt.ylabel('Count of note points')\n",
    "plt.box(False)\n",
    "plt.savefig('note_count_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for average length of note points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, noteeline_len, label='NoTeeline', marker='o', color=chosen_cmap(3))\n",
    "plt.plot(x, baseline_len, label='Baseline', marker='o', color=chosen_cmap(9))\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.35), ncol=2, frameon=False)\n",
    "plt.xticks([r+1 for r in range(len(pids))], pids)\n",
    "plt.ylabel('Avg. Length of note point')\n",
    "plt.box(False)\n",
    "plt.savefig('len_note_points_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for average writing time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, noteeline_time, label='NoTeeline', marker='o', color=chosen_cmap(3))\n",
    "plt.plot(x, baseline_time, label='Baseline', marker='o', color=chosen_cmap(9))\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.35), ncol=2, frameon=False)\n",
    "plt.xticks([r+1 for r in range(len(pids))], pids)\n",
    "plt.ylabel('Avg. Writing Time (seconds)')\n",
    "plt.box(False)\n",
    "plt.savefig('time_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e53a9-6411-4e98-9957-457e373bc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[70, 30])  # Adjusting width ratios\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "# Stacked Bar Plot on ax1\n",
    "index = np.arange(len(pids))  # the x locations for the groups\n",
    "bar_width = 0.35\n",
    "\n",
    "ax1.bar(index, noteeline_cnt, bar_width, label='NoTeeline', color=chosen_cmap(3), edgecolor='white')\n",
    "ax1.bar(index, baseline_cnt, bar_width, bottom=noteeline_cnt, label='Baseline', color=chosen_cmap(9), edgecolor='white')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Total # of Note Points')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticklabels(pids, rotation=90, ha='right')\n",
    "\n",
    "# Remove the box around ax1\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(False)\n",
    "# ax1.legend()\n",
    "ax1.legend(loc=\"lower left\", bbox_to_anchor=(0.3, -0.4), ncol=2, frameon=False, handlelength=1)\n",
    "\n",
    "# Box Plot on ax2\n",
    "data = [noteeline_cnt, baseline_cnt]\n",
    "bp = ax2.boxplot(data, patch_artist=True, positions=[1, 2], widths=0.2)\n",
    "\n",
    "# Setting colors for the box plot\n",
    "colors = [chosen_cmap(3), chosen_cmap(9)]\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Optionally, customize the colors of other components as well\n",
    "for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "    plt.setp(bp[element], color='black')\n",
    "\n",
    "ax2.set_xticklabels(['NoTeeline', 'Baseline'])\n",
    "ax2.set_ylabel('')\n",
    "# plt.box(False)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('note_count_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a8eb2-0d81-47d1-9cc2-2cdbd4fcf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[70, 30])  # Adjusting width ratios\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "# Stacked Bar Plot on ax1\n",
    "index = np.arange(len(pids))  # the x locations for the groups\n",
    "bar_width = 0.35\n",
    "\n",
    "ax1.bar(index, noteeline_len, bar_width, label='NoTeeline', color=chosen_cmap(3), edgecolor='white')\n",
    "ax1.bar(index, baseline_len, bar_width, bottom=noteeline_len, label='Baseline', color=chosen_cmap(9), edgecolor='white')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Average Note Length')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticklabels(pids, rotation=90, ha='right')\n",
    "\n",
    "# Remove the box around ax1\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(False)\n",
    "# ax1.legend()\n",
    "ax1.legend(loc=\"lower left\", bbox_to_anchor=(0.3, -0.4), ncol=2, frameon=False, handlelength=1)\n",
    "\n",
    "# Box Plot on ax2\n",
    "data = [noteeline_len, baseline_len]\n",
    "bp = ax2.boxplot(data, patch_artist=True, positions=[1, 2], widths=0.2)\n",
    "\n",
    "# Setting colors for the box plot\n",
    "colors = [chosen_cmap(3), chosen_cmap(9)]\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Optionally, customize the colors of other components as well\n",
    "for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "    plt.setp(bp[element], color='black')\n",
    "\n",
    "ax2.set_xticklabels(['NoTeeline', 'Baseline'])\n",
    "ax2.set_ylabel('')\n",
    "# plt.box(False)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('len_note_points_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfbb8a-bbcf-478c-80db-64a37baf8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[70, 30])  # Adjusting width ratios\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "# Stacked Bar Plot on ax1\n",
    "index = np.arange(len(pids))  # the x locations for the groups\n",
    "bar_width = 0.35\n",
    "\n",
    "ax1.bar(index, noteeline_time, bar_width, label='NoTeeline', color=chosen_cmap(3), edgecolor='white')\n",
    "ax1.bar(index, baseline_time, bar_width, bottom=noteeline_time, label='Baseline', color=chosen_cmap(9), edgecolor='white')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Average Writing Time')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticklabels(pids, rotation=90, ha='right')\n",
    "\n",
    "# Remove the box around ax1\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(False)\n",
    "# ax1.legend()\n",
    "ax1.legend(loc=\"lower left\", bbox_to_anchor=(0.3, -0.4), ncol=2, frameon=False, handlelength=1)\n",
    "\n",
    "# Box Plot on ax2\n",
    "data = [noteeline_time, baseline_time]\n",
    "bp = ax2.boxplot(data, patch_artist=True, positions=[1, 2], widths=0.2)\n",
    "\n",
    "# Setting colors for the box plot\n",
    "colors = [chosen_cmap(3), chosen_cmap(9)]\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Optionally, customize the colors of other components as well\n",
    "for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "    plt.setp(bp[element], color='black')\n",
    "\n",
    "ax2.set_xticklabels(['NoTeeline', 'Baseline'])\n",
    "ax2.set_ylabel('')\n",
    "# plt.box(False)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('time_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48125353-ba02-46b4-a8ee-51e858e3ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "user_dict = {}\n",
    "\n",
    "for user_folder in os.listdir(userlog_path):\n",
    "    print(f'Processing {user_folder}')\n",
    "    pd_dict = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        onboarding_data = json.load(f)\n",
    "                    pd_dict['ob_session'] = onboarding_data\n",
    "                else:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    \n",
    "                    if folder_number % 2 == 0:\n",
    "                        video_prefix = 'video2'\n",
    "                    else:\n",
    "                        video_prefix = 'video1'\n",
    "\n",
    "                    if file.lower().startswith(video_prefix):\n",
    "                        pd_dict['note_taking_time'] = [math.ceil(bpd['note_taking_time']/1000) for bpd in bullet_points_data['editHistory']]\n",
    "                        pd_dict['video_data'] = [\n",
    "                            {\n",
    "                                'point': bpd['point'],\n",
    "                                'expanded': bpd['edit'][-1][-1]['e_point']\n",
    "                            }\n",
    "                            for bpd in bullet_points_data['editHistory']\n",
    "                        ]\n",
    "                        user_dict[user_folder] = pd_dict\n",
    "\n",
    "print('\\n############ USER DATA ############')\n",
    "for user in user_dict:\n",
    "    print(f'{user} Data\\n----------------')\n",
    "    print('Onboarding Session\\n------------------')\n",
    "    print(f\"{user_dict[user]['ob_session']}\")\n",
    "    print('Note Taking Time\\n----------------')\n",
    "    print(f\"{user_dict[user]['note_taking_time']}\")\n",
    "    print('Notes and Corresponding Expansions\\n----------------------------------')\n",
    "    print(f\"{user_dict[user]['video_data']}\")\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc96533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "data = {user:user_dict[user]['note_taking_time'] for user in user_dict}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 4))  # Adjust the figure size as needed\n",
    "cmap = cm.get_cmap('Purples')  # Choose a colormap ('Blues' is an example)\n",
    "\n",
    "for person_id, time_differences in data.items():\n",
    "    start_time = 0\n",
    "    num_bars = len(time_differences)\n",
    "    # Adjust the range of normalized indices to avoid lightest shades\n",
    "    color_indices = [0.3 + 0.5 * i / (num_bars - 1) for i in range(num_bars)]\n",
    "    for i, time_diff in enumerate(time_differences):\n",
    "        end_time = start_time + time_diff\n",
    "        bar_color = cmap(color_indices[i])  # Get color from colormap based on normalized index\n",
    "        plt.barh(person_id, width=time_diff, left=start_time, height=0.1, color=bar_color, edgecolor='black')\n",
    "        start_time = end_time\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Person ID')\n",
    "plt.title('Time Intervals for Person IDs')\n",
    "plt.yticks(list(data.keys()), ['Person {}'.format(pid) for pid in data.keys()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a64438",
   "metadata": {},
   "source": [
    "# JS function `call_gpt` converted to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d892d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_prompt_string():\n",
    "    onboardings = [] # Assign values here...\n",
    "\n",
    "    # Simulating the check for existing onboardings\n",
    "    take_onboarding_into_prompt = bool(onboardings)\n",
    "\n",
    "    # Filter out onboardings with non-empty notes and non-empty keypoints\n",
    "    new_onboardings = [onboarding for onboarding in onboardings if onboarding['note'] and all(onboarding['keypoints'])]\n",
    "\n",
    "    # Update take_onboarding_into_prompt based on the filtered onboardings\n",
    "    if not new_onboardings:\n",
    "        take_onboarding_into_prompt = False\n",
    "\n",
    "    prompt_string = \"I want you to act as a personalized note-taking assistant. Users will give you a keypoint and the youtube transcript. \" + \\\n",
    "                    \"Your task is to expand the keypoint into a note point, by taking additional context from the transcript. The note should be a full sentence in simple english. \" + \\\n",
    "                    \"Follow these rules:\\n1. Resolve any typos or grammatical mistakes that arise in the keypoint.\\n2. The note should not be longer than 1 sentence. \" + \\\n",
    "                    \"3. Remember that the keypoint can be very abstract and as short as an abbreviation. Use the transcript to get additional information to ensure a good quality note expansion.\\n\" + \\\n",
    "                    \"4. Just write a single note point, users will request repeatedly for new points they want to add.\\n\" + \\\n",
    "                    \"5. Write it in a way a user would write in a notepad. Do not use sentences such as 'This video talks about...', 'The speaker explains..' etc.\"\n",
    "\n",
    "    if take_onboarding_into_prompt:\n",
    "        prompt_string += \"\\nMake sure that the note aligns with the user's writing style, so that they can read it easily. Use the same writing style as shown below.\\n\" + \\\n",
    "                         \"Here are three examples:\\n\"\n",
    "\n",
    "        for onboarding in new_onboardings:\n",
    "            prompt_string += \"Transcript: ...\" + onboarding['transcript'] + \"...\\n\" + \\\n",
    "                             \"Keypoint: \" + \", \".join(onboarding['keypoints']) + \"\\n\" + \\\n",
    "                             \"Note: \" + onboarding['note'] + \"\\n\\n\"\n",
    "\n",
    "        prompt_string += \"The keypoint refers to the high-level keypoint provided by the user and your task is to write a full 'Note' point. Make sure that your expanded note point matches the writing style of 'Note' in the provided examples.\"\n",
    "\n",
    "    return prompt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 20000\n",
    "\n",
    "def expand_point(point, transcript):\n",
    "    expanded_point = {'point': point['point'], 'transcript': []}\n",
    "\n",
    "    for line in transcript:\n",
    "        tr_offset = line['offset']\n",
    "        tr_end = line['offset'] + line['duration']\n",
    "        right = point['created_at'] * 1000.0  # converting to ms to match transcript time\n",
    "        left = right - WINDOW_SIZE  # Assuming WINDOW_SIZE is defined elsewhere\n",
    "\n",
    "        # there is partial or full overlapping between point and transcript\n",
    "        if not (right < tr_offset) and not (left > tr_end):\n",
    "            expanded_point['transcript'].append(line['text'])\n",
    "\n",
    "    return expanded_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Still incomplete\n",
    "    Needs some syntactical corrections...\n",
    "'''\n",
    "\n",
    "import openai # Might need to install this package\n",
    "\n",
    "async def call_gpt(points, transcription):\n",
    "    prompt_string = get_formatted_prompt_string()\n",
    "\n",
    "    expansion = []\n",
    "    for point in points:\n",
    "        if len(point['history']) > point['expand']:\n",
    "            expansion.append({'point': point['point'], 'expansion': point['history'][point['expand']], 'old': True})\n",
    "        else:\n",
    "            point_to_be_expanded = point['history'][point['expand'] - 1]\n",
    "            expanded_point = expand_point({'point': point_to_be_expanded, 'created_at': point['created_at'], 'utc_time': point['utc_time']}, transcription)\n",
    "            transcript = \".\".join(expanded_point['transcript'])\n",
    "            prompt = \"Expand the provided keypoint into a one sentence note.\\n\" + \\\n",
    "                     \"Transcript: ...\" + transcript + \"...\\n\" + \\\n",
    "                     \"Keypoint: \" + expanded_point['point'] + \"\\n\" + \\\n",
    "                     \"Note:\"\n",
    "\n",
    "            print('calling expansion from', prompt)\n",
    "\n",
    "            res = await openai.ChatCompletion.create(\n",
    "                messages=[{'role': 'system', 'content': prompt_string}, {'role': 'user', 'content': prompt}],\n",
    "                model=\"gpt-4-1106-preview\",\n",
    "                temperature=0.5\n",
    "            )\n",
    "\n",
    "            if res.choices[0].message.content is not None:\n",
    "                expansion.append({'point': point['point'], 'expansion': res.choices[0].message.content, 'old': False})\n",
    "\n",
    "    return expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb334f8-2a30-4f6b-ab18-4f10d64aa3de",
   "metadata": {},
   "source": [
    "# Stylometric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc61268-eb2a-47d8-aafd-5bb55d6a3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(auto_note)\n",
    "len(manual_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8626461-432f-4e4d-9124-d8fac69ed190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5807aa-50cb-4790-ba4b-284d449cfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the authors' corpora into lists of word tokens\n",
    "federalist_by_author_tokens = {}\n",
    "federalist_by_author_length_distributions = {}\n",
    "federalist_by_author_tokens_baseline = {}\n",
    "federalist_by_author_length_distributions_baseline = {}\n",
    "\n",
    "for author in range(0,12):\n",
    "    # ------------NoTeeline--------------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    tokens = nltk.word_tokenize(auto_note[author])\n",
    "    # filter out punctuation\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "    \n",
    "    federalist_by_author_tokens[author] = tokens\n",
    "    # Get a distribution of token lengths\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens[author]]\n",
    "    federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "    \n",
    "    token_lengths = [len(token) for token in tokens]\n",
    "    freq_dist = nltk.FreqDist(token_lengths)\n",
    "    lengths, frequencies = zip(*sorted(freq_dist.items()))\n",
    "    plt.plot(lengths, frequencies, label='Automatically expanded note in NoTeeline', color=chosen_cmap(3))\n",
    "\n",
    "\n",
    "    # ------------Baseline--------------\n",
    "\n",
    "    tokens = nltk.word_tokenize(manual_note[author])\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "    federalist_by_author_tokens_baseline[author] = tokens\n",
    "    # Get a distribution of token lengths\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens_baseline[author]]\n",
    "    federalist_by_author_length_distributions_baseline[author] = nltk.FreqDist(token_lengths)\n",
    "    \n",
    "    token_lengths = [len(token) for token in tokens]\n",
    "    freq_dist = nltk.FreqDist(token_lengths)\n",
    "    lengths, frequencies = zip(*sorted(freq_dist.items()))\n",
    "    plt.plot(lengths, frequencies, label='Manual note in Baseline', color=chosen_cmap(9))\n",
    "    label_ = author + 1\n",
    "    # plt.title(f'Token Length Distribution by Author {label_}')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.box(False)\n",
    "    plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.2), ncol=2, frameon=False)\n",
    "    plt.savefig('mcurve_p11.pdf', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22adb1-e738-40b1-809a-c6dd87d16a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in range(0,12):\n",
    "    federalist_by_author_tokens[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens[author]])\n",
    "\n",
    "    federalist_by_author_tokens_baseline[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens_baseline[author]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d444a7-22e4-4214-be8d-70b6b2cb40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate chisquared for each of the two candidate authors\n",
    "\n",
    "sum = 0\n",
    "for author in range(0,12):\n",
    "\n",
    "    # First, build a joint corpus and identify the 500 most frequent words in it\n",
    "    joint_corpus = (federalist_by_author_tokens[author] +\n",
    "                    federalist_by_author_tokens_baseline[author])\n",
    "    joint_freq_dist = nltk.FreqDist(joint_corpus)\n",
    "    most_common = list(joint_freq_dist.most_common(500))\n",
    "\n",
    "    # What proportion of the joint corpus is made up\n",
    "    # of the candidate author's tokens?\n",
    "    author_share = (len(federalist_by_author_tokens[author])\n",
    "                    / len(joint_corpus))\n",
    "\n",
    "    # Now, let's look at the 500 most common words in the candidate\n",
    "    # author's corpus and compare the number of times they can be observed\n",
    "    # to what would be expected if the author's papers\n",
    "    # and the Disputed papers were both random samples from the same distribution.\n",
    "    chisquared = 0\n",
    "    for word,joint_count in most_common:\n",
    "\n",
    "        # How often do we really see this common word?\n",
    "        author_count = federalist_by_author_tokens[author].count(word)\n",
    "        disputed_count = federalist_by_author_tokens_baseline[author].count(word)\n",
    "\n",
    "        # How often should we see it?\n",
    "        expected_author_count = joint_count * author_share\n",
    "        expected_disputed_count = joint_count * (1-author_share)\n",
    "\n",
    "        # Add the word's contribution to the chi-squared statistic\n",
    "        chisquared += ((author_count-expected_author_count) *\n",
    "                       (author_count-expected_author_count) /\n",
    "                       expected_author_count)\n",
    "\n",
    "        chisquared += ((disputed_count-expected_disputed_count) *\n",
    "                       (disputed_count-expected_disputed_count)\n",
    "                       / expected_disputed_count)\n",
    "\n",
    "    print(\"The Chi-squared statistic for candidate\", author+1, \"is\", chisquared)\n",
    "    sum += chisquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20208933-d06e-4626-a15e-a571893a7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff744ee0-ca39-4ac9-9e1e-ea76406f78fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
