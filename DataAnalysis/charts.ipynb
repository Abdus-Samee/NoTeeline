{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083867b-4edb-40b1-9ecb-a3ac860e3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: Dictionary where keys are person IDs and values are lists of time intervals\n",
    "data = {\n",
    "    1: [(10, 20), (30, 50), (60, 80)],  # For person ID 1, time intervals are (10, 20), (30, 50), and (60, 80)\n",
    "    2: [(15, 25), (40, 60), (70, 90)],  # For person ID 2, time intervals are (15, 25), (40, 60), and (70, 90)\n",
    "    3: [(5, 15), (35, 55), (65, 85)]    # For person ID 3, time intervals are (5, 15), (35, 55), and (65, 85)\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "\n",
    "for person_id, intervals in data.items():\n",
    "    for interval in intervals:\n",
    "        start, end = interval\n",
    "        plt.barh(person_id, width=end-start, left=start, height=0.1, color='#4338CA')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Person ID')\n",
    "plt.title('Time Intervals for Person IDs')\n",
    "plt.yticks(list(data.keys()), ['Person {}'.format(pid) for pid in data.keys()])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076300d-6c32-4301-9305-258de40e7746",
   "metadata": {},
   "source": [
    "# Create dictionary of user logs on video logistics and note logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c714254c-9159-4d45-850c-6877cba720b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing P1\n",
      "Processing P2\n",
      "Processing P3\n",
      "Processing P4\n",
      "Processing P5\n",
      "Processing P6\n",
      "Processing P7\n",
      "Processing P8\n",
      "Processing P9\n",
      "Processing P10\n",
      "Processing P11\n",
      "Processing P12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "data = []\n",
    "# pd = {}\n",
    "vd = {}\n",
    "auto_note = []\n",
    "manual_note = []\n",
    "\n",
    "# for user_folder in os.listdir(userlog_path):\n",
    "for folder_number in range(1, 13):\n",
    "    user_folder = f'P{folder_number}'\n",
    "    print(f'Processing {user_folder}')\n",
    "    user_data = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        # if folder_number % 2 == 0:\n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "\n",
    "                str_rep = ''\n",
    "                \n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    pass\n",
    "                # if not file.lower().startswith('video1') and not file.lower().startswith('video2'): continue\n",
    "                elif file.lower().startswith('video1') or file.lower().startswith('video2'):\n",
    "                    video_data = {}\n",
    "                    vd_data = {}\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    video_data['pauseCount'] = bullet_points_data['pauseCount']\n",
    "                    video_data['forwardCount'] = bullet_points_data['forwardCount']\n",
    "                    video_data['reverseCount'] = bullet_points_data['reverseCount']\n",
    "                    video_data['point_count'] = len(bullet_points_data['editHistory'])\n",
    "                    summary_t = bullet_points_data['summary_t']\n",
    "                    summary_p = bullet_points_data['summary_p']\n",
    "                    note_points = [\n",
    "                        {\n",
    "                            'point': bpd['point'], \n",
    "                            'time_taken': bpd['note_taking_time'],\n",
    "                            'timestamp': bpd['utc_time'],\n",
    "                            'expanded_note': bpd['edit'][-1][0]['e_point'] if len(bpd['edit']) > 1 else None,\n",
    "                            'transcript': bpd['fraction_transcript'],\n",
    "                            'v_id': file.lower()\n",
    "                        }\n",
    "                        for bpd in bullet_points_data['editHistory']\n",
    "                    ]\n",
    "\n",
    "                    for bpd in bullet_points_data['editHistory']:\n",
    "                        str_rep += bpd['edit'][-1][0]['e_point']\n",
    "\n",
    "                    vd_data['p_id'] = user_folder\n",
    "                    vd_data['note_points'] = note_points\n",
    "                    vd_data['summary_p'] = summary_p\n",
    "                    vd_data['summary_t'] = summary_t\n",
    "                    if folder_number % 2 == 0:\n",
    "                        if file.lower().startswith('video1'): \n",
    "                            user_data['Baseline'] = video_data\n",
    "                            vd_data['micronote'] = False\n",
    "                            if 'video1' not in vd:\n",
    "                                vd['video1'] = []\n",
    "                            vd['video1'].append(vd_data)\n",
    "                            manual_note.append(str_rep)\n",
    "                        elif file.lower().startswith('video2'): \n",
    "                            user_data['NoTeeline'] = video_data\n",
    "                            vd_data['micronote'] = True\n",
    "                            if 'video2' not in vd:\n",
    "                                vd['video2'] = []\n",
    "                            vd['video2'].append(vd_data)\n",
    "                            auto_note.append(str_rep)\n",
    "                    else:\n",
    "                        if file.lower().startswith('video1'): \n",
    "                            user_data['NoTeeline'] = video_data\n",
    "                            vd_data['micronote'] = True\n",
    "                            if 'video1' not in vd:\n",
    "                                vd['video1'] = []\n",
    "                            vd['video1'].append(vd_data)\n",
    "                            auto_note.append(str_rep)\n",
    "                        elif file.lower().startswith('video2'): \n",
    "                            user_data['Baseline'] = video_data\n",
    "                            vd_data['micronote'] = False\n",
    "                            if 'video2' not in vd:\n",
    "                                vd['video2'] = []\n",
    "                            vd['video2'].append(vd_data)\n",
    "                            manual_note.append(str_rep)\n",
    "    data.append(user_data)\n",
    "\n",
    "# print('############ COUNT DATA ############')\n",
    "# for i, d in enumerate(data):\n",
    "#     print('----------------------------------------')\n",
    "#     print(f'User {i+1}')\n",
    "#     print(d)\n",
    "\n",
    "# print('\\n############ VIDEO DATA ############')\n",
    "# for video in vd:\n",
    "#     print(f'--------------\\n{video} data...\\n--------------')\n",
    "#     for v in vd[video]:\n",
    "#         print('p_id\\n----')\n",
    "#         print(f\"{v['p_id']}\")\n",
    "#         print('micronote\\n---------')\n",
    "#         print(f\"{v['micronote']}\")\n",
    "#         print('summary_t\\n---------')\n",
    "#         print(f\"{v['summary_t']}\")\n",
    "#         print('summary_p\\n---------')\n",
    "#         print(f\"{v['summary_p']}\")\n",
    "#         print(f\"Notes:\\n------\\n{v['note_points']}\")\n",
    "#         print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7405a01-fbe4-4ebc-9dc5-37238afccd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_9440\\2194566083.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  chosen_cmap = matplotlib.cm.get_cmap('Paired')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "chosen_cmap = matplotlib.cm.get_cmap('Paired')\n",
    "# color_array = [chosen_cmap(i) for i in np.linspace(0, 1, 6)]\n",
    "# color_hex_array = [matplotlib.colors.to_hex(color) for color in color_array]\n",
    "\n",
    "# Lists to hold the plot data\n",
    "labels = []\n",
    "noteeline_pause = []\n",
    "noteeline_star_pause = []\n",
    "noteeline_forward = []\n",
    "noteeline_star_forward = []\n",
    "noteeline_reverse = []\n",
    "noteeline_star_reverse = []\n",
    "\n",
    "# Populate the lists with data\n",
    "for user, activities in enumerate(data):\n",
    "    # print(activities)\n",
    "    labels.append(f'P{user+1}')\n",
    "    noteeline_pause.append(activities['NoTeeline']['pauseCount'])\n",
    "    noteeline_star_pause.append(activities['Baseline']['pauseCount'])\n",
    "    noteeline_forward.append(activities['NoTeeline']['forwardCount'])\n",
    "    noteeline_star_forward.append(activities['Baseline']['forwardCount'])\n",
    "    noteeline_reverse.append(activities['NoTeeline']['reverseCount'])\n",
    "    noteeline_star_reverse.append(activities['Baseline']['reverseCount'])\n",
    "\n",
    "# Width of the bars\n",
    "barWidth = 0.3\n",
    "\n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Make the plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Stacked bar for 'noteeline'\n",
    "# bar1 = plt.bar(r1, noteeline_pause, color=chosen_cmap(1), width=barWidth, edgecolor='white', label='Pause (NoTeeline)')\n",
    "# bar2 = plt.bar(r1, noteeline_forward, bottom=noteeline_pause, color=chosen_cmap(3), width=barWidth, edgecolor='white', label='Seek Forward (NoTeeline)')\n",
    "# bar3 = plt.bar(r1, noteeline_reverse, bottom=[i+j for i,j in zip(noteeline_pause, noteeline_forward)], color=chosen_cmap(5), width=barWidth, edgecolor='white', label='Seek Backward (NoTeeline)')\n",
    "\n",
    "# # Stacked bar for 'noteeline*'\n",
    "# bar4 =plt.bar(r2, noteeline_star_pause, color=chosen_cmap(0), width=barWidth, edgecolor='white', label='Pause (Baseline)')\n",
    "# bar5 = plt.bar(r2, noteeline_star_forward, bottom=noteeline_star_pause, color=chosen_cmap(2), width=barWidth, edgecolor='white', label='Seek Forward (Baseline)')\n",
    "# bar6 = plt.bar(r2, noteeline_star_reverse, bottom=[i+j for i,j in zip(noteeline_star_pause, noteeline_star_forward)], color=chosen_cmap(4), width=barWidth, edgecolor='white', label='Seek Backword (Baseline)')\n",
    "\n",
    "# # Add xticks on the middle of the group bars\n",
    "# # plt.xlabel('User', fontweight='bold')\n",
    "# plt.xticks([r + barWidth/2 for r in range(len(labels))], labels)\n",
    "# plt.legend(loc=\"lower left\", ncol=2, bbox_to_anchor=(0.18, -0.3), frameon=False) # https://stackoverflow.com/a/54870844\n",
    "# plt.yticks([])  \n",
    "\n",
    "# # Create legend & Show graphic\n",
    "# # plt.legend()\n",
    "# plt.box(False)\n",
    "# plt.savefig('video_log_dist.pdf', bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e61c9",
   "metadata": {},
   "source": [
    "**Calculating Average of Metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c6bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noteeline average pause count: 9.583333333333334\n",
      "Noteeline average forward count: 0.0\n",
      "Noteeline average reverse count: 4.833333333333333\n"
     ]
    }
   ],
   "source": [
    "print(f'Noteeline average pause count: {sum(noteeline_star_pause) / len(noteeline_star_pause)}')\n",
    "print(f'Noteeline average forward count: {sum(noteeline_star_forward) / len(noteeline_star_forward)}')\n",
    "print(f'Noteeline average reverse count: {sum(noteeline_star_reverse) / len(noteeline_star_reverse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e04b8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noteeline metrics => mean: 0.5555555555555556, var: 3.469135802469136\n",
      "Noteeline metrics => mean: 4.944444444444445, var: 35.66358024691358\n"
     ]
    }
   ],
   "source": [
    "baseline = np.array([noteeline_star_pause, noteeline_star_forward, noteeline_star_reverse])\n",
    "noteeline = np.array([noteeline_pause, noteeline_forward, noteeline_reverse])\n",
    "\n",
    "print(f'Noteeline metrics => mean: {np.mean(noteeline)}, var: {np.var(noteeline)}')\n",
    "print(f'Noteeline metrics => mean: {np.mean(baseline)}, var: {np.var(baseline)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d40b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noteeline average pause count: 0.6666666666666666\n",
      "Noteeline average forward count: 0.0\n",
      "Noteeline average reverse count: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Noteeline average pause count: {sum(noteeline_pause) / len(noteeline_pause)}')\n",
    "print(f'Noteeline average forward count: {sum(noteeline_forward) / len(noteeline_forward)}')\n",
    "print(f'Noteeline average reverse count: {sum(noteeline_reverse) / len(noteeline_reverse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a2ac91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noteeline note length: 568.75\n",
      "Average baseline note length: 866.8333333333334\n"
     ]
    }
   ],
   "source": [
    "noteeline_sum = 0\n",
    "noteeline_len = 0\n",
    "baseline_sum = 0\n",
    "baseline_len = 0\n",
    "\n",
    "for v in vd:\n",
    "    for item in vd[v]:\n",
    "        if item['micronote']:\n",
    "            noteeline_sum += sum([len(s['point']) for s in item['note_points']])\n",
    "            noteeline_len += 1\n",
    "        else:\n",
    "            baseline_sum += sum([len(s['point']) for s in item['note_points']])\n",
    "            baseline_len += 1\n",
    "\n",
    "print(f'Average noteeline note length: {noteeline_sum / noteeline_len}')\n",
    "print(f'Average baseline note length: {baseline_sum / baseline_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc632253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noteeline note length: 30.333333333333332\n",
      "Variance of noteeline note length: 387.5555555555556\n",
      "Average baseline note length: 58.76836158192091\n",
      "Variance of baseline note length: 1299.059337993552\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "noteeline_lengths = [len(s['point']) for v in vd for item in vd[v] if item['micronote'] for s in item['note_points']]\n",
    "baseline_lengths = [len(s['point']) for v in vd for item in vd[v] if not item['micronote'] for s in item['note_points']]\n",
    "\n",
    "noteeline_mean_length = sum(noteeline_lengths) / len(noteeline_lengths)\n",
    "baseline_mean_length = sum(baseline_lengths) / len(baseline_lengths)\n",
    "\n",
    "noteeline_variance = np.var(noteeline_lengths)\n",
    "baseline_variance = np.var(baseline_lengths)\n",
    "\n",
    "print(f'Average noteeline note length: {noteeline_mean_length}')\n",
    "print(f'Variance of noteeline note length: {noteeline_variance}')\n",
    "print(f'Average baseline note length: {baseline_mean_length}')\n",
    "print(f'Variance of baseline note length: {baseline_variance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5216bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noteeline_cnt = []\n",
    "baseline_cnt = []\n",
    "noteeline_len = []\n",
    "baseline_len = []\n",
    "noteeline_time = []\n",
    "baseline_time = []\n",
    "pids = []\n",
    "x = []\n",
    "\n",
    "_iter = 1\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    pids.append(p_id)\n",
    "    x.append(_iter)\n",
    "    _iter += 1\n",
    "    for entry in entries:\n",
    "        if entry['tool'] == 'NoTeeline':\n",
    "            noteeline_cnt.append(entry['length'])\n",
    "            noteeline_len.append(entry['mean_point_length'])\n",
    "            noteeline_time.append(entry['mean_time_taken'])\n",
    "        else:\n",
    "            baseline_cnt.append(entry['length'])\n",
    "            baseline_len.append(entry['mean_point_length'])\n",
    "            baseline_time.append(entry['mean_time_taken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3039656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of noteeline counts: 32.354166666666664\n",
      "Variance of baseline counts: 6.6875\n",
      "Variance of noteeline lengths: 152.11318526182205\n",
      "Variance of baseline lengths: 287.0861080968939\n",
      "Variance of noteeline times: 29.04389609859831\n",
      "Variance of baseline times: 68.017726162181\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate variance for counts\n",
    "variance_noteeline_cnt = np.var(noteeline_cnt)\n",
    "variance_baseline_cnt = np.var(baseline_cnt)\n",
    "\n",
    "# Calculate variance for lengths\n",
    "variance_noteeline_len = np.var(noteeline_len)\n",
    "variance_baseline_len = np.var(baseline_len)\n",
    "\n",
    "# Calculate variance for times\n",
    "variance_noteeline_time = np.var(noteeline_time)\n",
    "variance_baseline_time = np.var(baseline_time)\n",
    "\n",
    "print(f'Variance of noteeline counts: {variance_noteeline_cnt}')\n",
    "print(f'Variance of baseline counts: {variance_baseline_cnt}')\n",
    "print(f'Variance of noteeline lengths: {variance_noteeline_len}')\n",
    "print(f'Variance of baseline lengths: {variance_baseline_len}')\n",
    "print(f'Variance of noteeline times: {variance_noteeline_time}')\n",
    "print(f'Variance of baseline times: {variance_baseline_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5fdcedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noteeline note length: 32.112151356269\n",
      "Average baseline note length: 60.63508862461068\n"
     ]
    }
   ],
   "source": [
    "print(f'Average noteeline note length: {sum(noteeline_len) / len(noteeline_len)}')\n",
    "print(f'Average baseline note length: {sum(baseline_len) / len(baseline_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23144f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average noteeline note count: 18.75\n",
      "Average baseline note count: 14.75\n"
     ]
    }
   ],
   "source": [
    "print(f'Average noteeline note count: {sum(noteeline_cnt) / len(noteeline_cnt)}')\n",
    "print(f'Average baseline note count: {sum(baseline_cnt) / len(baseline_cnt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4e85d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of noteeline time: 15.590165859302322 and variance: 29.04389609859831\n",
      "Average of baseline time: 27.786594371017447 and variance: 68.017726162181\n",
      "Percent decrease in average time: 43.8932110530123%\n",
      "Percent decrease in variance time: 57.29951920276453%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "noteeline_time_avg = np.mean(noteeline_time)\n",
    "noteeline_time_var = np.var(noteeline_time)\n",
    "baseline_time_avg = np.mean(baseline_time)\n",
    "baseline_time_var = np.var(baseline_time)\n",
    "\n",
    "print(f'Average of noteeline time: {noteeline_time_avg} and variance: {noteeline_time_var}')\n",
    "print(f'Average of baseline time: {baseline_time_avg} and variance: {baseline_time_var}')\n",
    "\n",
    "percent_decrease_avg = ((baseline_time_avg - noteeline_time_avg) / baseline_time_avg) * 100\n",
    "percent_decrease_var = ((baseline_time_var - noteeline_time_var) / baseline_time_var) * 100\n",
    "\n",
    "print(f'Percent decrease in average time: {percent_decrease_avg}%')\n",
    "print(f'Percent decrease in variance time: {percent_decrease_var}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f354e-c89f-4756-aa72-c54b3668807d",
   "metadata": {},
   "source": [
    "# Save Results as CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6117676b-877e-4269-9dd5-5cd485e0e182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_id</th>\n",
       "      <th>tool</th>\n",
       "      <th>micro_note</th>\n",
       "      <th>full_note</th>\n",
       "      <th>transcript</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>v_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>thinking about vivid memory</td>\n",
       "      <td>Recalling a vivid memory is easy compared to r...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>14268.461126</td>\n",
       "      <td>1.710088e+12</td>\n",
       "      <td>video1bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>then thinking about lunch from a while ago</td>\n",
       "      <td>The speaker highlights the difference in memor...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>7713.000000</td>\n",
       "      <td>1.710088e+12</td>\n",
       "      <td>video1bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>why do memories fade?</td>\n",
       "      <td>The video discusses the reasons behind the fad...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>5241.000000</td>\n",
       "      <td>1.710088e+12</td>\n",
       "      <td>video1bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>experience is converted to electricity</td>\n",
       "      <td>When you have an experience, such as dialing a...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>14938.000000</td>\n",
       "      <td>1.710088e+12</td>\n",
       "      <td>video1bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>goes to short term memory</td>\n",
       "      <td>When information is experienced, it is convert...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>5180.000000</td>\n",
       "      <td>1.710088e+12</td>\n",
       "      <td>video1bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>P12</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>Isolation also bad</td>\n",
       "      <td>Isolation associated with depression can lead ...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>7873.000000</td>\n",
       "      <td>1.710978e+12</td>\n",
       "      <td>video2bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>P12</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>Social old people slower memory decline</td>\n",
       "      <td>Older individuals with active social lives exp...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>6657.000000</td>\n",
       "      <td>1.710978e+12</td>\n",
       "      <td>video2bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>P12</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>Social interaction is workout, use it or lose it</td>\n",
       "      <td>Social interaction acts as a mental workout fo...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>11056.000000</td>\n",
       "      <td>1.710978e+12</td>\n",
       "      <td>video2bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>P12</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>Stay physically active, eat well</td>\n",
       "      <td>Staying physically active increases blood flow...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>13755.000000</td>\n",
       "      <td>1.710978e+12</td>\n",
       "      <td>video2bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>P12</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>Expose brain to challenges like new language</td>\n",
       "      <td>Challenging your brain by learning new languag...</td>\n",
       "      <td>[Think back to a really vivid memory., Got it?...</td>\n",
       "      <td>9502.000000</td>\n",
       "      <td>1.710978e+12</td>\n",
       "      <td>video2bulletpointsdata.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    p_id       tool                                        micro_note  \\\n",
       "0     P1  NoTeeline                       thinking about vivid memory   \n",
       "1     P1  NoTeeline        then thinking about lunch from a while ago   \n",
       "2     P1  NoTeeline                             why do memories fade?   \n",
       "3     P1  NoTeeline            experience is converted to electricity   \n",
       "4     P1  NoTeeline                         goes to short term memory   \n",
       "..   ...        ...                                               ...   \n",
       "397  P12  NoTeeline                                Isolation also bad   \n",
       "398  P12  NoTeeline           Social old people slower memory decline   \n",
       "399  P12  NoTeeline  Social interaction is workout, use it or lose it   \n",
       "400  P12  NoTeeline                  Stay physically active, eat well   \n",
       "401  P12  NoTeeline      Expose brain to challenges like new language   \n",
       "\n",
       "                                             full_note  \\\n",
       "0    Recalling a vivid memory is easy compared to r...   \n",
       "1    The speaker highlights the difference in memor...   \n",
       "2    The video discusses the reasons behind the fad...   \n",
       "3    When you have an experience, such as dialing a...   \n",
       "4    When information is experienced, it is convert...   \n",
       "..                                                 ...   \n",
       "397  Isolation associated with depression can lead ...   \n",
       "398  Older individuals with active social lives exp...   \n",
       "399  Social interaction acts as a mental workout fo...   \n",
       "400  Staying physically active increases blood flow...   \n",
       "401  Challenging your brain by learning new languag...   \n",
       "\n",
       "                                            transcript    time_taken  \\\n",
       "0    [Think back to a really vivid memory., Got it?...  14268.461126   \n",
       "1    [Think back to a really vivid memory., Got it?...   7713.000000   \n",
       "2    [Think back to a really vivid memory., Got it?...   5241.000000   \n",
       "3    [Think back to a really vivid memory., Got it?...  14938.000000   \n",
       "4    [Think back to a really vivid memory., Got it?...   5180.000000   \n",
       "..                                                 ...           ...   \n",
       "397  [Think back to a really vivid memory., Got it?...   7873.000000   \n",
       "398  [Think back to a really vivid memory., Got it?...   6657.000000   \n",
       "399  [Think back to a really vivid memory., Got it?...  11056.000000   \n",
       "400  [Think back to a really vivid memory., Got it?...  13755.000000   \n",
       "401  [Think back to a really vivid memory., Got it?...   9502.000000   \n",
       "\n",
       "        timestamp                         v_id  \n",
       "0    1.710088e+12  video1bulletpointsdata.json  \n",
       "1    1.710088e+12  video1bulletpointsdata.json  \n",
       "2    1.710088e+12  video1bulletpointsdata.json  \n",
       "3    1.710088e+12  video1bulletpointsdata.json  \n",
       "4    1.710088e+12  video1bulletpointsdata.json  \n",
       "..            ...                          ...  \n",
       "397  1.710978e+12  video2bulletpointsdata.json  \n",
       "398  1.710978e+12  video2bulletpointsdata.json  \n",
       "399  1.710978e+12  video2bulletpointsdata.json  \n",
       "400  1.710978e+12  video2bulletpointsdata.json  \n",
       "401  1.710978e+12  video2bulletpointsdata.json  \n",
       "\n",
       "[402 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the note_leng_log dictionary\n",
    "note_leng_log = {}\n",
    "\n",
    "# Loop over videos and their data\n",
    "for video in vd:\n",
    "    # print(f'--------------\\n{video} data...\\n--------------')\n",
    "    for v in vd[video]:\n",
    "        # print('p_id\\n----')\n",
    "        # print(f\"{v['p_id']}\")\n",
    "        # print('micronote\\n---------')\n",
    "        # print(f\"{v['micronote']}\")\n",
    "\n",
    "        if v['micronote']: \n",
    "            for item in v['note_points']:\n",
    "                # Prepare a dictionary for the current entry\n",
    "                current_entry = {\n",
    "                    'tool': 'NoTeeline',\n",
    "                    'micro_note': item['point'],\n",
    "                    'full_note': item['expanded_note'],\n",
    "                    'transcript': item['transcript'],\n",
    "                    'time_taken': item['time_taken'],\n",
    "                    'timestamp': item['timestamp'],\n",
    "                    'v_id': item['v_id']\n",
    "                }\n",
    "                # Append or create a dict item in note_leng_log for the current p_id\n",
    "                if v['p_id'] in note_leng_log:\n",
    "                    note_leng_log[v['p_id']].append(current_entry)\n",
    "                else:\n",
    "                    note_leng_log[v['p_id']] = [current_entry]\n",
    "        else:\n",
    "            for item in v['note_points']:\n",
    "                # Prepare a dictionary for the current entry\n",
    "                current_entry = {\n",
    "                    'tool': 'Baseline',\n",
    "                    'micro_note': item['point'],\n",
    "                    'full_note': '',\n",
    "                    'transcript': item['transcript']}\n",
    "\n",
    "                # Append or create a dict item in note_leng_log for the current p_id\n",
    "                if v['p_id'] in note_leng_log:\n",
    "                    note_leng_log[v['p_id']].append(current_entry)\n",
    "                else:\n",
    "                    note_leng_log[v['p_id']] = [current_entry]\n",
    "\n",
    "# 'note_leng_log' now contains the required log information\n",
    "\n",
    "# Flatten the note_leng_log dict to a list of dicts, ensuring p_id is the first key\n",
    "flat_data = []\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    for entry in entries:\n",
    "        # Construct a new dictionary with p_id as the first key\n",
    "        entry_with_pid = {'p_id': p_id, **entry}\n",
    "        flat_data.append(entry_with_pid)\n",
    "\n",
    "# Convert the flat_data list of dicts to a pandas DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# Now p_id will be the first column in the DataFrame\n",
    "# print(df.head())\n",
    "df.to_csv('all_points.csv', index=False) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9330e-a159-453e-aa26-e178d22952f2",
   "metadata": {},
   "source": [
    "# Summary Log Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fe9d2-e34b-46af-afb7-63201351e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "data = []\n",
    "total_ = 0\n",
    "wrong_ = 0\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key='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',\n",
    ")\n",
    "\n",
    "def generate_point_summary(points: str, context: str) -> str:\n",
    "    user_prompt = (\n",
    "        f'''I will give you a context and some keypoints, Your task is to summarize the keypoints in 4 sentences.\n",
    "        Focus on the keypoint, only use context if you need extra information:\n",
    "        Context: {context}\n",
    "        Keypoints: {points}\n",
    "        Remember not to make it too long.\n",
    "        Do not mark the sentences with 1,2 etc.'''\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",  # Make sure to use the correct model name\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=150,  # Adjust max_tokens if necessary\n",
    "        # seed=SEED,  # Optionally, set a seed for deterministic output, if needed\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# for user_folder in os.listdir(userlog_path):\n",
    "for folder_number in range(1, 13):\n",
    "    user_folder = f'P{folder_number}'\n",
    "    print(f'Processing {user_folder}')\n",
    "    user_data = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        \n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    continue\n",
    "                # if not file.lower().startswith('video1') and not file.lower().startswith('video2'): continue\n",
    "                else:\n",
    "                    vd_data = {}\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    \n",
    "                    note_points = ''\n",
    "                    for bpd in bullet_points_data['editHistory']:\n",
    "                        note_points += bpd['edit'][-1][0]['e_point']\n",
    "                        total_ += 1\n",
    "\n",
    "                        # checking error\n",
    "                        if len(bpd['edit']) > 1:\n",
    "                            expanded_note_string = bpd['edit'][-1][0]['e_point']\n",
    "                            if 'sorry' in expanded_note_string.lower():\n",
    "                                print(expanded_note_string)\n",
    "                                wrong_ += 1\n",
    "                            elif 'please' in expanded_note_string.lower():\n",
    "                                print(expanded_note_string)\n",
    "                                wrong_ += 1\n",
    "                            elif 'not found' in expanded_note_string.lower():\n",
    "                                print(expanded_note_string)\n",
    "                                wrong_ += 1\n",
    "            \n",
    "\n",
    "                    summary_t = bullet_points_data['summary_t']\n",
    "                    if bullet_points_data['summary_p'] == '':\n",
    "                        if bullet_points_data['summary_t'] != '':\n",
    "                            summary_p = generate_point_summary(note_points, summary_t)\n",
    "                            print(user_folder, file)\n",
    "                            print(summary_p)\n",
    "                    else:\n",
    "                        summary_p = bullet_points_data['summary_p']\n",
    "\n",
    "                        \n",
    "                    vd_data['p_id'] = user_folder\n",
    "                    vd_data['file'] = file\n",
    "                    vd_data['note_points'] = note_points\n",
    "                    vd_data['summary_p'] = summary_p\n",
    "                    vd_data['summary_t'] = summary_t\n",
    "\n",
    "                    data.append(vd_data)\n",
    "\n",
    "print(total_, wrong_)\n",
    "df = pd.DataFrame(data)\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9416a16-db54-4c43-9ae0-ac94d234b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "video1 data...\n",
      "--------------\n",
      "p_id\n",
      "----\n",
      "P1\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P2\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P3\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P4\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P5\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P6\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P7\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P8\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P9\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P10\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P11\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P12\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "--------------\n",
      "video2 data...\n",
      "--------------\n",
      "p_id\n",
      "----\n",
      "P1\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P2\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P3\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P4\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P5\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P6\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P7\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P8\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P9\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P10\n",
      "micronote\n",
      "---------\n",
      "True\n",
      "p_id\n",
      "----\n",
      "P11\n",
      "micronote\n",
      "---------\n",
      "False\n",
      "p_id\n",
      "----\n",
      "P12\n",
      "micronote\n",
      "---------\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_id</th>\n",
       "      <th>tool</th>\n",
       "      <th>length</th>\n",
       "      <th>mean_time_taken</th>\n",
       "      <th>std_time_taken</th>\n",
       "      <th>mean_point_length</th>\n",
       "      <th>std_point_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>21</td>\n",
       "      <td>11.523451</td>\n",
       "      <td>7.111831</td>\n",
       "      <td>31.047619</td>\n",
       "      <td>11.441263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>17</td>\n",
       "      <td>37.987466</td>\n",
       "      <td>20.103353</td>\n",
       "      <td>76.176471</td>\n",
       "      <td>45.546660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P2</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>21</td>\n",
       "      <td>15.902476</td>\n",
       "      <td>24.463894</td>\n",
       "      <td>14.809524</td>\n",
       "      <td>9.781513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P2</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>17</td>\n",
       "      <td>16.440196</td>\n",
       "      <td>9.978936</td>\n",
       "      <td>25.941176</td>\n",
       "      <td>14.210479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P3</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>18</td>\n",
       "      <td>16.351478</td>\n",
       "      <td>8.435625</td>\n",
       "      <td>33.222222</td>\n",
       "      <td>10.659141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P3</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>11</td>\n",
       "      <td>38.649341</td>\n",
       "      <td>20.908779</td>\n",
       "      <td>73.181818</td>\n",
       "      <td>34.332116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P4</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>12</td>\n",
       "      <td>28.768500</td>\n",
       "      <td>23.767411</td>\n",
       "      <td>65.166667</td>\n",
       "      <td>35.479650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P4</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>15</td>\n",
       "      <td>17.609598</td>\n",
       "      <td>8.537676</td>\n",
       "      <td>58.866667</td>\n",
       "      <td>35.942299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P5</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>15</td>\n",
       "      <td>16.624500</td>\n",
       "      <td>6.471961</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>19.734910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P5</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>12</td>\n",
       "      <td>27.723316</td>\n",
       "      <td>14.216546</td>\n",
       "      <td>72.250000</td>\n",
       "      <td>45.122288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P6</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>15</td>\n",
       "      <td>21.423868</td>\n",
       "      <td>11.994213</td>\n",
       "      <td>60.533333</td>\n",
       "      <td>21.715944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P6</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>22</td>\n",
       "      <td>12.591030</td>\n",
       "      <td>6.642755</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>6.286710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P7</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>13</td>\n",
       "      <td>21.621445</td>\n",
       "      <td>9.221104</td>\n",
       "      <td>42.153846</td>\n",
       "      <td>24.327402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P7</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>13</td>\n",
       "      <td>43.443453</td>\n",
       "      <td>31.163029</td>\n",
       "      <td>64.153846</td>\n",
       "      <td>27.573248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P8</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>14</td>\n",
       "      <td>30.888718</td>\n",
       "      <td>33.375911</td>\n",
       "      <td>44.928571</td>\n",
       "      <td>28.310685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P8</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>21</td>\n",
       "      <td>11.700573</td>\n",
       "      <td>6.407942</td>\n",
       "      <td>34.809524</td>\n",
       "      <td>17.812834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P9</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>28</td>\n",
       "      <td>8.624213</td>\n",
       "      <td>5.830889</td>\n",
       "      <td>12.571429</td>\n",
       "      <td>5.460433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>P9</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>15</td>\n",
       "      <td>23.516269</td>\n",
       "      <td>13.396735</td>\n",
       "      <td>70.800000</td>\n",
       "      <td>29.450976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>P10</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>16</td>\n",
       "      <td>23.878037</td>\n",
       "      <td>8.721663</td>\n",
       "      <td>47.812500</td>\n",
       "      <td>22.077191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P10</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>15</td>\n",
       "      <td>17.868494</td>\n",
       "      <td>10.787128</td>\n",
       "      <td>14.733333</td>\n",
       "      <td>3.567757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P11</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>10</td>\n",
       "      <td>28.040687</td>\n",
       "      <td>9.327123</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>11.670904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P11</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>15</td>\n",
       "      <td>23.848416</td>\n",
       "      <td>13.741388</td>\n",
       "      <td>60.933333</td>\n",
       "      <td>28.995325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>P12</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>16</td>\n",
       "      <td>17.409272</td>\n",
       "      <td>6.510078</td>\n",
       "      <td>76.875000</td>\n",
       "      <td>30.977562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P12</td>\n",
       "      <td>NoTeeline</td>\n",
       "      <td>30</td>\n",
       "      <td>8.086325</td>\n",
       "      <td>3.292515</td>\n",
       "      <td>29.200000</td>\n",
       "      <td>11.335490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_id       tool  length  mean_time_taken  std_time_taken  \\\n",
       "0    P1  NoTeeline      21        11.523451        7.111831   \n",
       "1    P1   Baseline      17        37.987466       20.103353   \n",
       "2    P2   Baseline      21        15.902476       24.463894   \n",
       "3    P2  NoTeeline      17        16.440196        9.978936   \n",
       "4    P3  NoTeeline      18        16.351478        8.435625   \n",
       "5    P3   Baseline      11        38.649341       20.908779   \n",
       "6    P4   Baseline      12        28.768500       23.767411   \n",
       "7    P4  NoTeeline      15        17.609598        8.537676   \n",
       "8    P5  NoTeeline      15        16.624500        6.471961   \n",
       "9    P5   Baseline      12        27.723316       14.216546   \n",
       "10   P6   Baseline      15        21.423868       11.994213   \n",
       "11   P6  NoTeeline      22        12.591030        6.642755   \n",
       "12   P7  NoTeeline      13        21.621445        9.221104   \n",
       "13   P7   Baseline      13        43.443453       31.163029   \n",
       "14   P8   Baseline      14        30.888718       33.375911   \n",
       "15   P8  NoTeeline      21        11.700573        6.407942   \n",
       "16   P9  NoTeeline      28         8.624213        5.830889   \n",
       "17   P9   Baseline      15        23.516269       13.396735   \n",
       "18  P10   Baseline      16        23.878037        8.721663   \n",
       "19  P10  NoTeeline      15        17.868494       10.787128   \n",
       "20  P11  NoTeeline      10        28.040687        9.327123   \n",
       "21  P11   Baseline      15        23.848416       13.741388   \n",
       "22  P12   Baseline      16        17.409272        6.510078   \n",
       "23  P12  NoTeeline      30         8.086325        3.292515   \n",
       "\n",
       "    mean_point_length  std_point_length  \n",
       "0           31.047619         11.441263  \n",
       "1           76.176471         45.546660  \n",
       "2           14.809524          9.781513  \n",
       "3           25.941176         14.210479  \n",
       "4           33.222222         10.659141  \n",
       "5           73.181818         34.332116  \n",
       "6           65.166667         35.479650  \n",
       "7           58.866667         35.942299  \n",
       "8           45.000000         19.734910  \n",
       "9           72.250000         45.122288  \n",
       "10          60.533333         21.715944  \n",
       "11          22.500000          6.286710  \n",
       "12          42.153846         24.327402  \n",
       "13          64.153846         27.573248  \n",
       "14          44.928571         28.310685  \n",
       "15          34.809524         17.812834  \n",
       "16          12.571429          5.460433  \n",
       "17          70.800000         29.450976  \n",
       "18          47.812500         22.077191  \n",
       "19          14.733333          3.567757  \n",
       "20          35.300000         11.670904  \n",
       "21          60.933333         28.995325  \n",
       "22          76.875000         30.977562  \n",
       "23          29.200000         11.335490  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the note_leng_log dictionary\n",
    "note_leng_log = {}\n",
    "\n",
    "# Loop over videos and their data\n",
    "for video in vd:\n",
    "    print(f'--------------\\n{video} data...\\n--------------')\n",
    "    for v in vd[video]:\n",
    "        print('p_id\\n----')\n",
    "        print(f\"{v['p_id']}\")\n",
    "        print('micronote\\n---------')\n",
    "        print(f\"{v['micronote']}\")\n",
    "\n",
    "        # Calculate statistics based on the provided 'data'\n",
    "        time_taken_values = [item['time_taken']/1000 for item in v['note_points']]\n",
    "        mean_time_taken = np.mean(time_taken_values)\n",
    "        std_time_taken = np.std(time_taken_values)\n",
    "        \n",
    "        point_lengths = [len(item['point']) for item in v['note_points']]\n",
    "        mean_point_length = np.mean(point_lengths)\n",
    "        std_point_length = np.std(point_lengths)\n",
    "\n",
    "        # Prepare a dictionary for the current entry\n",
    "        current_entry = {\n",
    "            'tool': 'NoTeeline' if v['micronote'] else 'Baseline',\n",
    "            'length': len(v['note_points']),\n",
    "            'mean_time_taken': mean_time_taken,\n",
    "            'std_time_taken': std_time_taken,\n",
    "            'mean_point_length': mean_point_length,\n",
    "            'std_point_length': std_point_length\n",
    "        }\n",
    "\n",
    "        # Append or create a dict item in note_leng_log for the current p_id\n",
    "        if v['p_id'] in note_leng_log:\n",
    "            note_leng_log[v['p_id']].append(current_entry)\n",
    "        else:\n",
    "            note_leng_log[v['p_id']] = [current_entry]\n",
    "\n",
    "# 'note_leng_log' now contains the required log information\n",
    "\n",
    "# Flatten the note_leng_log dict to a list of dicts, ensuring p_id is the first key\n",
    "flat_data = []\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    for entry in entries:\n",
    "        # Construct a new dictionary with p_id as the first key\n",
    "        entry_with_pid = {'p_id': p_id, **entry}\n",
    "        flat_data.append(entry_with_pid)\n",
    "\n",
    "# Convert the flat_data list of dicts to a pandas DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# Now p_id will be the first column in the DataFrame\n",
    "# print(df.head())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1979b-233e-4666-80d7-5a3b4edaa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Assuming chosen_cmap is defined somewhere else in your code, for example:\n",
    "# chosen_cmap = cm.get_cmap('YourColormapName')\n",
    "\n",
    "noteeline_cnt = []\n",
    "baseline_cnt = []\n",
    "noteeline_len = []\n",
    "baseline_len = []\n",
    "noteeline_time = []\n",
    "baseline_time = []\n",
    "pids = []\n",
    "x = []\n",
    "\n",
    "_iter = 1\n",
    "for p_id, entries in note_leng_log.items():\n",
    "    pids.append(p_id)\n",
    "    x.append(_iter)\n",
    "    _iter += 1\n",
    "    for entry in entries:\n",
    "        if entry['tool'] == 'NoTeeline':\n",
    "            noteeline_cnt.append(entry['length'])\n",
    "            noteeline_len.append(entry['mean_point_length'])\n",
    "            noteeline_time.append(entry['mean_time_taken'])\n",
    "        else:\n",
    "            baseline_cnt.append(entry['length'])\n",
    "            baseline_len.append(entry['mean_point_length'])\n",
    "            baseline_time.append(entry['mean_time_taken'])\n",
    "\n",
    "# Define font sizes for different parts of the plot\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 26\n",
    "\n",
    "# Set font sizes\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# Plot for count of notes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, noteeline_cnt, label='NoTeeline', marker='o', color=chosen_cmap(3))\n",
    "plt.plot(x, baseline_cnt, label='Baseline', marker='o', color=chosen_cmap(9))\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.35), ncol=2, frameon=False)\n",
    "plt.xticks([r+1 for r in range(len(pids))], pids)\n",
    "plt.ylabel('Count of note points')\n",
    "plt.box(False)\n",
    "plt.savefig('note_count_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for average length of note points\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, noteeline_len, label='NoTeeline', marker='o', color=chosen_cmap(3))\n",
    "plt.plot(x, baseline_len, label='Baseline', marker='o', color=chosen_cmap(9))\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.35), ncol=2, frameon=False)\n",
    "plt.xticks([r+1 for r in range(len(pids))], pids)\n",
    "plt.ylabel('Avg. Length of note point')\n",
    "plt.box(False)\n",
    "plt.savefig('len_note_points_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Plot for average writing time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, noteeline_time, label='NoTeeline', marker='o', color=chosen_cmap(3))\n",
    "plt.plot(x, baseline_time, label='Baseline', marker='o', color=chosen_cmap(9))\n",
    "plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.35), ncol=2, frameon=False)\n",
    "plt.xticks([r+1 for r in range(len(pids))], pids)\n",
    "plt.ylabel('Avg. Writing Time (seconds)')\n",
    "plt.box(False)\n",
    "plt.savefig('time_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e53a9-6411-4e98-9957-457e373bc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[70, 30])  # Adjusting width ratios\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "# Stacked Bar Plot on ax1\n",
    "index = np.arange(len(pids))  # the x locations for the groups\n",
    "bar_width = 0.35\n",
    "\n",
    "ax1.bar(index, noteeline_cnt, bar_width, label='NoTeeline', color=chosen_cmap(3), edgecolor='white')\n",
    "ax1.bar(index, baseline_cnt, bar_width, bottom=noteeline_cnt, label='Baseline', color=chosen_cmap(9), edgecolor='white')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Total # of Note Points')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticklabels(pids, rotation=90, ha='right')\n",
    "\n",
    "# Remove the box around ax1\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(False)\n",
    "# ax1.legend()\n",
    "ax1.legend(loc=\"lower left\", bbox_to_anchor=(0.3, -0.4), ncol=2, frameon=False, handlelength=1)\n",
    "\n",
    "# Box Plot on ax2\n",
    "data = [noteeline_cnt, baseline_cnt]\n",
    "bp = ax2.boxplot(data, patch_artist=True, positions=[1, 2], widths=0.2)\n",
    "\n",
    "# Setting colors for the box plot\n",
    "colors = [chosen_cmap(3), chosen_cmap(9)]\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Optionally, customize the colors of other components as well\n",
    "for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "    plt.setp(bp[element], color='black')\n",
    "\n",
    "ax2.set_xticklabels(['NoTeeline', 'Baseline'])\n",
    "ax2.set_ylabel('')\n",
    "# plt.box(False)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('note_count_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a8eb2-0d81-47d1-9cc2-2cdbd4fcf4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[70, 30])  # Adjusting width ratios\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "# Stacked Bar Plot on ax1\n",
    "index = np.arange(len(pids))  # the x locations for the groups\n",
    "bar_width = 0.35\n",
    "\n",
    "ax1.bar(index, noteeline_len, bar_width, label='NoTeeline', color=chosen_cmap(3), edgecolor='white')\n",
    "ax1.bar(index, baseline_len, bar_width, bottom=noteeline_len, label='Baseline', color=chosen_cmap(9), edgecolor='white')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Average Note Length')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticklabels(pids, rotation=90, ha='right')\n",
    "\n",
    "# Remove the box around ax1\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(False)\n",
    "# ax1.legend()\n",
    "ax1.legend(loc=\"lower left\", bbox_to_anchor=(0.3, -0.4), ncol=2, frameon=False, handlelength=1)\n",
    "\n",
    "# Box Plot on ax2\n",
    "data = [noteeline_len, baseline_len]\n",
    "bp = ax2.boxplot(data, patch_artist=True, positions=[1, 2], widths=0.2)\n",
    "\n",
    "# Setting colors for the box plot\n",
    "colors = [chosen_cmap(3), chosen_cmap(9)]\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Optionally, customize the colors of other components as well\n",
    "for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "    plt.setp(bp[element], color='black')\n",
    "\n",
    "ax2.set_xticklabels(['NoTeeline', 'Baseline'])\n",
    "ax2.set_ylabel('')\n",
    "# plt.box(False)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('len_note_points_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dfbb8a-bbcf-478c-80db-64a37baf8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[70, 30])  # Adjusting width ratios\n",
    "\n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "# Stacked Bar Plot on ax1\n",
    "index = np.arange(len(pids))  # the x locations for the groups\n",
    "bar_width = 0.35\n",
    "\n",
    "ax1.bar(index, noteeline_time, bar_width, label='NoTeeline', color=chosen_cmap(3), edgecolor='white')\n",
    "ax1.bar(index, baseline_time, bar_width, bottom=noteeline_time, label='Baseline', color=chosen_cmap(9), edgecolor='white')\n",
    "\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_ylabel('Average Writing Time')\n",
    "ax1.set_xticks(index)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticklabels(pids, rotation=90, ha='right')\n",
    "\n",
    "# Remove the box around ax1\n",
    "for spine in ax1.spines.values():\n",
    "    spine.set_visible(False)\n",
    "# ax1.legend()\n",
    "ax1.legend(loc=\"lower left\", bbox_to_anchor=(0.3, -0.4), ncol=2, frameon=False, handlelength=1)\n",
    "\n",
    "# Box Plot on ax2\n",
    "data = [noteeline_time, baseline_time]\n",
    "bp = ax2.boxplot(data, patch_artist=True, positions=[1, 2], widths=0.2)\n",
    "\n",
    "# Setting colors for the box plot\n",
    "colors = [chosen_cmap(3), chosen_cmap(9)]\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Optionally, customize the colors of other components as well\n",
    "for element in ['whiskers', 'caps', 'medians', 'fliers']:\n",
    "    plt.setp(bp[element], color='black')\n",
    "\n",
    "ax2.set_xticklabels(['NoTeeline', 'Baseline'])\n",
    "ax2.set_ylabel('')\n",
    "# plt.box(False)\n",
    "# plt.tight_layout()\n",
    "plt.savefig('time_dist.pdf', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48125353-ba02-46b4-a8ee-51e858e3ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "user_dict = {}\n",
    "\n",
    "for user_folder in os.listdir(userlog_path):\n",
    "    print(f'Processing {user_folder}')\n",
    "    pd_dict = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        onboarding_data = json.load(f)\n",
    "                    pd_dict['ob_session'] = onboarding_data\n",
    "                else:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    \n",
    "                    if folder_number % 2 == 0:\n",
    "                        video_prefix = 'video2'\n",
    "                    else:\n",
    "                        video_prefix = 'video1'\n",
    "\n",
    "                    if file.lower().startswith(video_prefix):\n",
    "                        pd_dict['note_taking_time'] = [math.ceil(bpd['note_taking_time']/1000) for bpd in bullet_points_data['editHistory']]\n",
    "                        pd_dict['video_data'] = [\n",
    "                            {\n",
    "                                'point': bpd['point'],\n",
    "                                'expanded': bpd['edit'][-1][-1]['e_point']\n",
    "                            }\n",
    "                            for bpd in bullet_points_data['editHistory']\n",
    "                        ]\n",
    "                        user_dict[user_folder] = pd_dict\n",
    "\n",
    "print('\\n############ USER DATA ############')\n",
    "for user in user_dict:\n",
    "    print(f'{user} Data\\n----------------')\n",
    "    print('Onboarding Session\\n------------------')\n",
    "    print(f\"{user_dict[user]['ob_session']}\")\n",
    "    print('Note Taking Time\\n----------------')\n",
    "    print(f\"{user_dict[user]['note_taking_time']}\")\n",
    "    print('Notes and Corresponding Expansions\\n----------------------------------')\n",
    "    print(f\"{user_dict[user]['video_data']}\")\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc96533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "data = {user:user_dict[user]['note_taking_time'] for user in user_dict}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 4))  # Adjust the figure size as needed\n",
    "cmap = cm.get_cmap('Purples')  # Choose a colormap ('Blues' is an example)\n",
    "\n",
    "for person_id, time_differences in data.items():\n",
    "    start_time = 0\n",
    "    num_bars = len(time_differences)\n",
    "    # Adjust the range of normalized indices to avoid lightest shades\n",
    "    color_indices = [0.3 + 0.5 * i / (num_bars - 1) for i in range(num_bars)]\n",
    "    for i, time_diff in enumerate(time_differences):\n",
    "        end_time = start_time + time_diff\n",
    "        bar_color = cmap(color_indices[i])  # Get color from colormap based on normalized index\n",
    "        plt.barh(person_id, width=time_diff, left=start_time, height=0.1, color=bar_color, edgecolor='black')\n",
    "        start_time = end_time\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Person ID')\n",
    "plt.title('Time Intervals for Person IDs')\n",
    "plt.yticks(list(data.keys()), ['Person {}'.format(pid) for pid in data.keys()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a64438",
   "metadata": {},
   "source": [
    "# JS function `call_gpt` converted to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d892d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_prompt_string():\n",
    "    onboardings = [] # Assign values here...\n",
    "\n",
    "    # Simulating the check for existing onboardings\n",
    "    take_onboarding_into_prompt = bool(onboardings)\n",
    "\n",
    "    # Filter out onboardings with non-empty notes and non-empty keypoints\n",
    "    new_onboardings = [onboarding for onboarding in onboardings if onboarding['note'] and all(onboarding['keypoints'])]\n",
    "\n",
    "    # Update take_onboarding_into_prompt based on the filtered onboardings\n",
    "    if not new_onboardings:\n",
    "        take_onboarding_into_prompt = False\n",
    "\n",
    "    prompt_string = \"I want you to act as a personalized note-taking assistant. Users will give you a keypoint and the youtube transcript. \" + \\\n",
    "                    \"Your task is to expand the keypoint into a note point, by taking additional context from the transcript. The note should be a full sentence in simple english. \" + \\\n",
    "                    \"Follow these rules:\\n1. Resolve any typos or grammatical mistakes that arise in the keypoint.\\n2. The note should not be longer than 1 sentence. \" + \\\n",
    "                    \"3. Remember that the keypoint can be very abstract and as short as an abbreviation. Use the transcript to get additional information to ensure a good quality note expansion.\\n\" + \\\n",
    "                    \"4. Just write a single note point, users will request repeatedly for new points they want to add.\\n\" + \\\n",
    "                    \"5. Write it in a way a user would write in a notepad. Do not use sentences such as 'This video talks about...', 'The speaker explains..' etc.\"\n",
    "\n",
    "    if take_onboarding_into_prompt:\n",
    "        prompt_string += \"\\nMake sure that the note aligns with the user's writing style, so that they can read it easily. Use the same writing style as shown below.\\n\" + \\\n",
    "                         \"Here are three examples:\\n\"\n",
    "\n",
    "        for onboarding in new_onboardings:\n",
    "            prompt_string += \"Transcript: ...\" + onboarding['transcript'] + \"...\\n\" + \\\n",
    "                             \"Keypoint: \" + \", \".join(onboarding['keypoints']) + \"\\n\" + \\\n",
    "                             \"Note: \" + onboarding['note'] + \"\\n\\n\"\n",
    "\n",
    "        prompt_string += \"The keypoint refers to the high-level keypoint provided by the user and your task is to write a full 'Note' point. Make sure that your expanded note point matches the writing style of 'Note' in the provided examples.\"\n",
    "\n",
    "    return prompt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1156bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 20000\n",
    "\n",
    "def expand_point(point, transcript):\n",
    "    expanded_point = {'point': point['point'], 'transcript': []}\n",
    "\n",
    "    for line in transcript:\n",
    "        tr_offset = line['offset']\n",
    "        tr_end = line['offset'] + line['duration']\n",
    "        right = point['created_at'] * 1000.0  # converting to ms to match transcript time\n",
    "        left = right - WINDOW_SIZE  # Assuming WINDOW_SIZE is defined elsewhere\n",
    "\n",
    "        # there is partial or full overlapping between point and transcript\n",
    "        if not (right < tr_offset) and not (left > tr_end):\n",
    "            expanded_point['transcript'].append(line['text'])\n",
    "\n",
    "    return expanded_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Still incomplete\n",
    "    Needs some syntactical corrections...\n",
    "'''\n",
    "\n",
    "import openai # Might need to install this package\n",
    "\n",
    "async def call_gpt(points, transcription):\n",
    "    prompt_string = get_formatted_prompt_string()\n",
    "\n",
    "    expansion = []\n",
    "    for point in points:\n",
    "        if len(point['history']) > point['expand']:\n",
    "            expansion.append({'point': point['point'], 'expansion': point['history'][point['expand']], 'old': True})\n",
    "        else:\n",
    "            point_to_be_expanded = point['history'][point['expand'] - 1]\n",
    "            expanded_point = expand_point({'point': point_to_be_expanded, 'created_at': point['created_at'], 'utc_time': point['utc_time']}, transcription)\n",
    "            transcript = \".\".join(expanded_point['transcript'])\n",
    "            prompt = \"Expand the provided keypoint into a one sentence note.\\n\" + \\\n",
    "                     \"Transcript: ...\" + transcript + \"...\\n\" + \\\n",
    "                     \"Keypoint: \" + expanded_point['point'] + \"\\n\" + \\\n",
    "                     \"Note:\"\n",
    "\n",
    "            print('calling expansion from', prompt)\n",
    "\n",
    "            res = await openai.ChatCompletion.create(\n",
    "                messages=[{'role': 'system', 'content': prompt_string}, {'role': 'user', 'content': prompt}],\n",
    "                model=\"gpt-4-1106-preview\",\n",
    "                temperature=0.5\n",
    "            )\n",
    "\n",
    "            if res.choices[0].message.content is not None:\n",
    "                expansion.append({'point': point['point'], 'expansion': res.choices[0].message.content, 'old': False})\n",
    "\n",
    "    return expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb334f8-2a30-4f6b-ab18-4f10d64aa3de",
   "metadata": {},
   "source": [
    "# Stylometric analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc61268-eb2a-47d8-aafd-5bb55d6a3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(auto_note)\n",
    "len(manual_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8626461-432f-4e4d-9124-d8fac69ed190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5807aa-50cb-4790-ba4b-284d449cfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the authors' corpora into lists of word tokens\n",
    "federalist_by_author_tokens = {}\n",
    "federalist_by_author_length_distributions = {}\n",
    "federalist_by_author_tokens_baseline = {}\n",
    "federalist_by_author_length_distributions_baseline = {}\n",
    "\n",
    "for author in range(0,12):\n",
    "    # ------------NoTeeline--------------\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    tokens = nltk.word_tokenize(auto_note[author])\n",
    "    # filter out punctuation\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "    \n",
    "    federalist_by_author_tokens[author] = tokens\n",
    "    # Get a distribution of token lengths\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens[author]]\n",
    "    federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "    \n",
    "    token_lengths = [len(token) for token in tokens]\n",
    "    freq_dist = nltk.FreqDist(token_lengths)\n",
    "    lengths, frequencies = zip(*sorted(freq_dist.items()))\n",
    "    plt.plot(lengths, frequencies, label='Automatically expanded note in NoTeeline', color=chosen_cmap(3))\n",
    "\n",
    "\n",
    "    # ------------Baseline--------------\n",
    "\n",
    "    tokens = nltk.word_tokenize(manual_note[author])\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "    federalist_by_author_tokens_baseline[author] = tokens\n",
    "    # Get a distribution of token lengths\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens_baseline[author]]\n",
    "    federalist_by_author_length_distributions_baseline[author] = nltk.FreqDist(token_lengths)\n",
    "    \n",
    "    token_lengths = [len(token) for token in tokens]\n",
    "    freq_dist = nltk.FreqDist(token_lengths)\n",
    "    lengths, frequencies = zip(*sorted(freq_dist.items()))\n",
    "    plt.plot(lengths, frequencies, label='Manual note in Baseline', color=chosen_cmap(9))\n",
    "    label_ = author + 1\n",
    "    # plt.title(f'Token Length Distribution by Author {label_}')\n",
    "    plt.xlabel('Token Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.box(False)\n",
    "    plt.legend(loc=\"lower left\", bbox_to_anchor=(0.1, -0.2), ncol=2, frameon=False)\n",
    "    plt.savefig('mcurve_p11.pdf', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22adb1-e738-40b1-809a-c6dd87d16a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in range(0,12):\n",
    "    federalist_by_author_tokens[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens[author]])\n",
    "\n",
    "    federalist_by_author_tokens_baseline[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens_baseline[author]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d444a7-22e4-4214-be8d-70b6b2cb40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate chisquared for each of the two candidate authors\n",
    "\n",
    "sum = 0\n",
    "for author in range(0,12):\n",
    "\n",
    "    # First, build a joint corpus and identify the 500 most frequent words in it\n",
    "    joint_corpus = (federalist_by_author_tokens[author] +\n",
    "                    federalist_by_author_tokens_baseline[author])\n",
    "    joint_freq_dist = nltk.FreqDist(joint_corpus)\n",
    "    most_common = list(joint_freq_dist.most_common(500))\n",
    "\n",
    "    # What proportion of the joint corpus is made up\n",
    "    # of the candidate author's tokens?\n",
    "    author_share = (len(federalist_by_author_tokens[author])\n",
    "                    / len(joint_corpus))\n",
    "\n",
    "    # Now, let's look at the 500 most common words in the candidate\n",
    "    # author's corpus and compare the number of times they can be observed\n",
    "    # to what would be expected if the author's papers\n",
    "    # and the Disputed papers were both random samples from the same distribution.\n",
    "    chisquared = 0\n",
    "    for word,joint_count in most_common:\n",
    "\n",
    "        # How often do we really see this common word?\n",
    "        author_count = federalist_by_author_tokens[author].count(word)\n",
    "        disputed_count = federalist_by_author_tokens_baseline[author].count(word)\n",
    "\n",
    "        # How often should we see it?\n",
    "        expected_author_count = joint_count * author_share\n",
    "        expected_disputed_count = joint_count * (1-author_share)\n",
    "\n",
    "        # Add the word's contribution to the chi-squared statistic\n",
    "        chisquared += ((author_count-expected_author_count) *\n",
    "                       (author_count-expected_author_count) /\n",
    "                       expected_author_count)\n",
    "\n",
    "        chisquared += ((disputed_count-expected_disputed_count) *\n",
    "                       (disputed_count-expected_disputed_count)\n",
    "                       / expected_disputed_count)\n",
    "\n",
    "    print(\"The Chi-squared statistic for candidate\", author+1, \"is\", chisquared)\n",
    "    sum += chisquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20208933-d06e-4626-a15e-a571893a7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff744ee0-ca39-4ac9-9e1e-ea76406f78fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
