{
  "buttonStats": {
    "theme_count": 1,
    "expand_count": 3,
    "time_count": 0
  },
  "pauseCount": 3,
  "forwardCount": 15,
  "reverseCount": 1,
  "summary_t": "The lecture provides a comprehensive overview of self-attention mechanisms and transformers in NLP, highlighting their advantages over RNNs, such as improved handling of long-range dependencies and increased parallelization capabilities. It delves into the transformer architecture, explaining its components like key, query, value attention, multi-headed attention, and the use of residual connections, layer normalization, and scaled dot-product attention for enhanced training. Practical aspects such as sequence order encoding and preventing future information leakage in sequence prediction are also discussed.\n\nThe lecturer expands on multi-headed attention, explaining how it allows the model to focus on various parts of the input sequence simultaneously by using multiple sets of query, key, and value matrices, each learning different transformations. This enables the model to capture a richer representation of the input. The importance of residual connections in aiding training by allowing gradients to flow through the network is emphasized, along with the role of layer normalization in stabilizing training by normalizing the inputs across features.\n\nFurthermore, the lecture addresses the computational challenges of transformers, particularly the quadratic complexity of self-attention with respect to sequence length, which becomes problematic for very long sequences. Recent advancements such as Linformer and other methods that reduce this complexity by projecting to lower-dimensional spaces or combining local window attention with random and global attention are introduced.\n\nFinally, the lecture reminds students of the significance of upcoming assignments and the final project proposal, encouraging them to consider the discussed concepts and advancements in their work. The session concludes with an open floor for questions, touching on topics like the scaling factor in dot-product attention and potential use cases where RNNs might outperform transformers.\n\nThe refined summary incorporates additional details on multi-headed attention, the importance of residual connections and layer normalization, as well as the challenges and recent solutions to the quadratic computational complexity of transformers.",
  "summary_p": "In the lecture, the use of bidirectional LSTM in encoding is highlighted, with the attention mechanism enabling the decoder to utilize encoded representations as a memory form, focusing on relevant parts at any given time. This approach is foundational in sequence to sequence models, particularly in creating end-to-end differentiable machine translation systems, where an encoder and decoder framework is pivotal. The lecture also touches upon the nature of recurrent neural networks (RNNs), which can be unrolled in both directions, thus capturing linear locality and maintaining a computational complexity that is linear with respect to sequence length. This linear encoding of locality by RNNs contrasts with the quadratic computational complexity challenges faced by transformers, underscoring the evolution and adaptation within NLP technologies.",
  "url": "www.youtube.com/watch?v=ptuGllU5SQQ",
  "editHistory": [
    {
      "point": "encoding with bidirectional LSTM, attention from the decoder given back as memory",
      "fraction_transcript": [
        null,
        "Hi, everyone.",
        "Welcome to CS224N, lecture\nnine, Self Attention",
        "and Transformers.",
        "If I am not able to\nbe heard right now,",
        "please someone send\na message in the chat",
        "because I can't see anyone.",
        "But I'm excited to get\ninto the content for today,",
        "we'll be talking about self\nattention and transformers.",
        "Let us dive into\nthe lecture plan",
        "and we'll talk about some\nsort of to do's for the course",
        "as well.",
        "So we'll start with\nwhere we were back",
        "last week with recurrence,\nrecurrent neural networks",
        "and we'll talk about a\nmovement from recurrence",
        "to attention based\non NLP models,",
        "we talked about\nattention and we're going",
        "to just go all in on attention.",
        "We'll introduce the\ntransformer model,",
        "which is a particular type of\nattention based model that's",
        "very popular, you need to know\nit, you're going to learn it.",
        "We'll talk about\nsome great results",
        "with transformers and then\nsome drawbacks and variants",
        "and sort of very recent\nwork on improving them.",
        "So some reminders before we\njump in, assignment 4 is due,",
        "the mid-quarter feedback survey\nis due Tuesday, February 16th.",
        "You get some small number\nof points for doing that",
        "and we really\nappreciate your feedback",
        "on what we've done well,\nwhat we can improve on.",
        "And then final project\nproposal is also due,",
        "one note on the proposals, part\nof the goal of the proposal",
        "is to, I'd say the main part\nof the goal of the proposal",
        "is to give you feedback on the\nidea that you have presented",
        "and make sure that\nit is a viable option",
        "for a final project and make\nsure we kind of recenter",
        "if not.",
        "And so we want to get feedback\nto you very quickly on that,",
        "OK.",
        "All right.",
        "So with that, let's\nstart in on the content",
        "of this week's lecture.",
        "So we were in this place\nin NLP as of last week,",
        "where we had recurrent\nneural networks,",
        "sort of for a lot of things\nthat you wanted to do.",
        "So it's around 2016\nand the strategy",
        "if you want to build a strong\nand healthy model, is you have",
        "sentences that\nyou need to encode",
        "and you have a\nbidirectional LSTM say,",
        "and maybe it looks a little\nbit like this pictographically",
        "and maybe it's a source\nsentence in a translation,",
        "for example, we saw\nmachine translation.",
        "And then you define your\noutput, which is maybe",
        "a sequence of words which is the\ntarget translation that we're",
        "trying to predict or\nmaybe it's a parse tree,",
        "or it's a summary, and you\nuse an LSTM with one direction",
        "to generate it.",
        "And this works really well.",
        "We use these architectures\nto do all kinds",
        "of interesting things, but\none thing that we said,",
        "we talked about is information\nsort of bottleneck that you're",
        "trying to encode, maybe a\nvery long sequence in sort",
        "of the very last vector in your,\nor one vector in your encoder,",
        "and so we use the\ntension as this mechanism",
        "to take a representation from\nour decoder and sort of look",
        "back and treat the encoded\nrepresentations as a memory,",
        "that we can reference\nand sort of pick out",
        "what's important to any given\ntime, and that was attention.",
        "And this week, we're going to\ndo something slightly different."
      ],
      "utc_time": 1710626835927,
      "note_taking_time": 214020.00587983703,
      "edit": [
        [
          {
            "e_point": "encoding with bidirectional LSTM, attention from the decoder given back as memory",
            "e_time": 1710626835927
          }
        ],
        [
          {
            "e_point": "Encoding is done with bidirectional LSTM, and the attention mechanism allows the decoder to treat encoded representations as a form of memory to reference and highlight what's important at any time.",
            "e_time": 1710626984064
          }
        ]
      ]
    },
    {
      "point": "seq2seq - encoder and decoder",
      "fraction_transcript": [
        null,
        "Hi, everyone.",
        "Welcome to CS224N, lecture\nnine, Self Attention",
        "and Transformers.",
        "If I am not able to\nbe heard right now,",
        "please someone send\na message in the chat",
        "because I can't see anyone.",
        "But I'm excited to get\ninto the content for today,",
        "we'll be talking about self\nattention and transformers.",
        "Let us dive into\nthe lecture plan",
        "and we'll talk about some\nsort of to do's for the course",
        "as well.",
        "So we'll start with\nwhere we were back",
        "last week with recurrence,\nrecurrent neural networks",
        "and we'll talk about a\nmovement from recurrence",
        "to attention based\non NLP models,",
        "we talked about\nattention and we're going",
        "to just go all in on attention.",
        "We'll introduce the\ntransformer model,",
        "which is a particular type of\nattention based model that's",
        "very popular, you need to know\nit, you're going to learn it.",
        "We'll talk about\nsome great results",
        "with transformers and then\nsome drawbacks and variants",
        "and sort of very recent\nwork on improving them.",
        "So some reminders before we\njump in, assignment 4 is due,",
        "the mid-quarter feedback survey\nis due Tuesday, February 16th.",
        "You get some small number\nof points for doing that",
        "and we really\nappreciate your feedback",
        "on what we've done well,\nwhat we can improve on.",
        "And then final project\nproposal is also due,",
        "one note on the proposals, part\nof the goal of the proposal",
        "is to, I'd say the main part\nof the goal of the proposal",
        "is to give you feedback on the\nidea that you have presented",
        "and make sure that\nit is a viable option",
        "for a final project and make\nsure we kind of recenter",
        "if not.",
        "And so we want to get feedback\nto you very quickly on that,",
        "OK.",
        "All right.",
        "So with that, let's\nstart in on the content",
        "of this week's lecture.",
        "So we were in this place\nin NLP as of last week,",
        "where we had recurrent\nneural networks,",
        "sort of for a lot of things\nthat you wanted to do.",
        "So it's around 2016\nand the strategy",
        "if you want to build a strong\nand healthy model, is you have",
        "sentences that\nyou need to encode",
        "and you have a\nbidirectional LSTM say,",
        "and maybe it looks a little\nbit like this pictographically",
        "and maybe it's a source\nsentence in a translation,",
        "for example, we saw\nmachine translation.",
        "And then you define your\noutput, which is maybe",
        "a sequence of words which is the\ntarget translation that we're",
        "trying to predict or\nmaybe it's a parse tree,",
        "or it's a summary, and you\nuse an LSTM with one direction",
        "to generate it.",
        "And this works really well.",
        "We use these architectures\nto do all kinds",
        "of interesting things, but\none thing that we said,",
        "we talked about is information\nsort of bottleneck that you're",
        "trying to encode, maybe a\nvery long sequence in sort",
        "of the very last vector in your,\nor one vector in your encoder,",
        "and so we use the\ntension as this mechanism",
        "to take a representation from\nour decoder and sort of look",
        "back and treat the encoded\nrepresentations as a memory,",
        "that we can reference\nand sort of pick out",
        "what's important to any given\ntime, and that was attention.",
        "And this week, we're going to\ndo something slightly different.",
        "So we learned about sequence to\nsequence models, the encoder,",
        "decoder way of thinking\nabout problems, more or less",
        "in order to deal with this\nidea of building a machine",
        "translation system that's end\nto end differentiable, right?",
        "And so this is sort of\na really interesting way"
      ],
      "utc_time": 1710626851741,
      "note_taking_time": 15814,
      "edit": [
        [
          {
            "e_point": "seq2seq - encoder and decoder",
            "e_time": 1710626851741
          }
        ],
        [
          {
            "e_point": "Sequence to sequence models employ an encoder and decoder framework to handle tasks like building an end-to-end differentiable machine translation system.",
            "e_time": 1710626984064
          }
        ]
      ]
    },
    {
      "point": "RNNs are unrolled l to r or opp ",
      "fraction_transcript": [
        null,
        "Hi, everyone.",
        "Welcome to CS224N, lecture\nnine, Self Attention",
        "and Transformers.",
        "If I am not able to\nbe heard right now,",
        "please someone send\na message in the chat",
        "because I can't see anyone.",
        "But I'm excited to get\ninto the content for today,",
        "we'll be talking about self\nattention and transformers.",
        "Let us dive into\nthe lecture plan",
        "and we'll talk about some\nsort of to do's for the course",
        "as well.",
        "So we'll start with\nwhere we were back",
        "last week with recurrence,\nrecurrent neural networks",
        "and we'll talk about a\nmovement from recurrence",
        "to attention based\non NLP models,",
        "we talked about\nattention and we're going",
        "to just go all in on attention.",
        "We'll introduce the\ntransformer model,",
        "which is a particular type of\nattention based model that's",
        "very popular, you need to know\nit, you're going to learn it.",
        "We'll talk about\nsome great results",
        "with transformers and then\nsome drawbacks and variants",
        "and sort of very recent\nwork on improving them.",
        "So some reminders before we\njump in, assignment 4 is due,",
        "the mid-quarter feedback survey\nis due Tuesday, February 16th.",
        "You get some small number\nof points for doing that",
        "and we really\nappreciate your feedback",
        "on what we've done well,\nwhat we can improve on.",
        "And then final project\nproposal is also due,",
        "one note on the proposals, part\nof the goal of the proposal",
        "is to, I'd say the main part\nof the goal of the proposal",
        "is to give you feedback on the\nidea that you have presented",
        "and make sure that\nit is a viable option",
        "for a final project and make\nsure we kind of recenter",
        "if not.",
        "And so we want to get feedback\nto you very quickly on that,",
        "OK.",
        "All right.",
        "So with that, let's\nstart in on the content",
        "of this week's lecture.",
        "So we were in this place\nin NLP as of last week,",
        "where we had recurrent\nneural networks,",
        "sort of for a lot of things\nthat you wanted to do.",
        "So it's around 2016\nand the strategy",
        "if you want to build a strong\nand healthy model, is you have",
        "sentences that\nyou need to encode",
        "and you have a\nbidirectional LSTM say,",
        "and maybe it looks a little\nbit like this pictographically",
        "and maybe it's a source\nsentence in a translation,",
        "for example, we saw\nmachine translation.",
        "And then you define your\noutput, which is maybe",
        "a sequence of words which is the\ntarget translation that we're",
        "trying to predict or\nmaybe it's a parse tree,",
        "or it's a summary, and you\nuse an LSTM with one direction",
        "to generate it.",
        "And this works really well.",
        "We use these architectures\nto do all kinds",
        "of interesting things, but\none thing that we said,",
        "we talked about is information\nsort of bottleneck that you're",
        "trying to encode, maybe a\nvery long sequence in sort",
        "of the very last vector in your,\nor one vector in your encoder,",
        "and so we use the\ntension as this mechanism",
        "to take a representation from\nour decoder and sort of look",
        "back and treat the encoded\nrepresentations as a memory,",
        "that we can reference\nand sort of pick out",
        "what's important to any given\ntime, and that was attention.",
        "And this week, we're going to\ndo something slightly different.",
        "So we learned about sequence to\nsequence models, the encoder,",
        "decoder way of thinking\nabout problems, more or less",
        "in order to deal with this\nidea of building a machine",
        "translation system that's end\nto end differentiable, right?",
        "And so this is sort of\na really interesting way",
        "of thinking about problems.",
        "What we'll do this\nweek is different.",
        "We're not trying\nto motivate sort",
        "of an entirely new way of\nthinking about problems",
        "like machine translation,\ninstead we're",
        "going to take the building\nblocks that we were using,",
        "recurrent neural\nnetworks and we're",
        "going to spend a lot\nof trial and error",
        "in the field trying to figure\nout if there are building",
        "blocks that just work\nbetter across a broad range",
        "of problems, sort of\nslot the new thing",
        "in for recurrent neural\nnetworks and say,",
        "voila, maybe it works better.",
        "And so I want to take us\non this sort of journey",
        "to self attention\nnetworks, and we'll",
        "start with some problems with\nrecurrent neural networks.",
        "So we spent a bit of time\ntrying to convince you",
        "that recurrent neural\nnetworks were very useful.",
        "Now I'm going to\ntalk about reasons",
        "why they can be improved.",
        "So we know that recurrent\nnetworks are enrolled",
        "left to right in air quotes, it\ncould be right to left as well.",
        "So what does this mean?",
        "A recurrent neural network\nencodes linear locality, right?",
        "So once I'm looking at\ntasty in this phrase,",
        "I'm about to look\nat Pizza or if I'm"
      ],
      "utc_time": 1710626910846,
      "note_taking_time": 59105,
      "edit": [
        [
          {
            "e_point": "RNNs are unrolled l to r or opp ",
            "e_time": 1710626910846
          }
        ],
        [
          {
            "e_point": "Recurrent neural networks (RNNs) can be unrolled either from left to right or from right to left, encoding linear locality.",
            "e_time": 1710626984064
          }
        ]
      ]
    },
    {
      "point": "RNNS encode linear locality, linear complexity on seq length ",
      "fraction_transcript": [
        null,
        "Hi, everyone.",
        "Welcome to CS224N, lecture\nnine, Self Attention",
        "and Transformers.",
        "If I am not able to\nbe heard right now,",
        "please someone send\na message in the chat",
        "because I can't see anyone.",
        "But I'm excited to get\ninto the content for today,",
        "we'll be talking about self\nattention and transformers.",
        "Let us dive into\nthe lecture plan",
        "and we'll talk about some\nsort of to do's for the course",
        "as well.",
        "So we'll start with\nwhere we were back",
        "last week with recurrence,\nrecurrent neural networks",
        "and we'll talk about a\nmovement from recurrence",
        "to attention based\non NLP models,",
        "we talked about\nattention and we're going",
        "to just go all in on attention.",
        "We'll introduce the\ntransformer model,",
        "which is a particular type of\nattention based model that's",
        "very popular, you need to know\nit, you're going to learn it.",
        "We'll talk about\nsome great results",
        "with transformers and then\nsome drawbacks and variants",
        "and sort of very recent\nwork on improving them.",
        "So some reminders before we\njump in, assignment 4 is due,",
        "the mid-quarter feedback survey\nis due Tuesday, February 16th.",
        "You get some small number\nof points for doing that",
        "and we really\nappreciate your feedback",
        "on what we've done well,\nwhat we can improve on.",
        "And then final project\nproposal is also due,",
        "one note on the proposals, part\nof the goal of the proposal",
        "is to, I'd say the main part\nof the goal of the proposal",
        "is to give you feedback on the\nidea that you have presented",
        "and make sure that\nit is a viable option",
        "for a final project and make\nsure we kind of recenter",
        "if not.",
        "And so we want to get feedback\nto you very quickly on that,",
        "OK.",
        "All right.",
        "So with that, let's\nstart in on the content",
        "of this week's lecture.",
        "So we were in this place\nin NLP as of last week,",
        "where we had recurrent\nneural networks,",
        "sort of for a lot of things\nthat you wanted to do.",
        "So it's around 2016\nand the strategy",
        "if you want to build a strong\nand healthy model, is you have",
        "sentences that\nyou need to encode",
        "and you have a\nbidirectional LSTM say,",
        "and maybe it looks a little\nbit like this pictographically",
        "and maybe it's a source\nsentence in a translation,",
        "for example, we saw\nmachine translation.",
        "And then you define your\noutput, which is maybe",
        "a sequence of words which is the\ntarget translation that we're",
        "trying to predict or\nmaybe it's a parse tree,",
        "or it's a summary, and you\nuse an LSTM with one direction",
        "to generate it.",
        "And this works really well.",
        "We use these architectures\nto do all kinds",
        "of interesting things, but\none thing that we said,",
        "we talked about is information\nsort of bottleneck that you're",
        "trying to encode, maybe a\nvery long sequence in sort",
        "of the very last vector in your,\nor one vector in your encoder,",
        "and so we use the\ntension as this mechanism",
        "to take a representation from\nour decoder and sort of look",
        "back and treat the encoded\nrepresentations as a memory,",
        "that we can reference\nand sort of pick out",
        "what's important to any given\ntime, and that was attention.",
        "And this week, we're going to\ndo something slightly different.",
        "So we learned about sequence to\nsequence models, the encoder,",
        "decoder way of thinking\nabout problems, more or less",
        "in order to deal with this\nidea of building a machine",
        "translation system that's end\nto end differentiable, right?",
        "And so this is sort of\na really interesting way",
        "of thinking about problems.",
        "What we'll do this\nweek is different.",
        "We're not trying\nto motivate sort",
        "of an entirely new way of\nthinking about problems",
        "like machine translation,\ninstead we're",
        "going to take the building\nblocks that we were using,",
        "recurrent neural\nnetworks and we're",
        "going to spend a lot\nof trial and error",
        "in the field trying to figure\nout if there are building",
        "blocks that just work\nbetter across a broad range",
        "of problems, sort of\nslot the new thing",
        "in for recurrent neural\nnetworks and say,",
        "voila, maybe it works better.",
        "And so I want to take us\non this sort of journey",
        "to self attention\nnetworks, and we'll",
        "start with some problems with\nrecurrent neural networks.",
        "So we spent a bit of time\ntrying to convince you",
        "that recurrent neural\nnetworks were very useful.",
        "Now I'm going to\ntalk about reasons",
        "why they can be improved.",
        "So we know that recurrent\nnetworks are enrolled",
        "left to right in air quotes, it\ncould be right to left as well.",
        "So what does this mean?",
        "A recurrent neural network\nencodes linear locality, right?",
        "So once I'm looking at\ntasty in this phrase,",
        "I'm about to look\nat Pizza or if I'm",
        "going in the other direction\nonce I look at Pizza,",
        "I'm about to look at tasty.",
        "And so it's very easy\nfor their meanings,",
        "for their presence\nin the sentence",
        "to affect the meaning, to\naffect the representation",
        "of the other word.",
        "And this is actually\nquite useful",
        "because nearby words frequently\ndo influence each other, that's",
        "practically one of\nthe things we talked",
        "about with the distributional\nhypothesis as encoded",
        "by something like Word2vec.",
        "But if words are\ndistant linearly,",
        "they can still interact\nwith each other.",
        "This is something that we\nsaw in dependency parsing.",
        "So if I have say the phrase the\nchef, notice chef bolded here,",
        "I'm running a recurrent neural\nnetwork over this, and then",
        "the chef who, and we're going\nto have this long sequence",
        "that I'm going to encode.",
        "And then the word\nwas, right, maybe it",
        "is the chef who was,\nbut in between I",
        "have O of sequence length many\nsteps of the computation that I"
      ],
      "utc_time": 1710626967845,
      "note_taking_time": 56999,
      "edit": [
        [
          {
            "e_point": "RNNS encode linear locality, linear complexity on seq length ",
            "e_time": 1710626967845
          }
        ],
        [
          {
            "e_point": "Recurrent neural networks encode linear locality with a computational complexity that is linear in terms of the sequence length.",
            "e_time": 1710626984064
          }
        ]
      ]
    }
  ]
}