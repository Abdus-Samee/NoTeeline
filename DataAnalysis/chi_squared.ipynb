{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing P1\n",
      "Processing P2\n",
      "Processing P3\n",
      "Processing P4\n",
      "Processing P5\n",
      "Processing P6\n",
      "Processing P7\n",
      "Processing P8\n",
      "Processing P9\n",
      "Processing P10\n",
      "Processing P11\n",
      "Processing P12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "userlog_path = \"UserLog\"\n",
    "data = []\n",
    "# pd = {}\n",
    "vd = {}\n",
    "auto_note = []\n",
    "manual_note = []\n",
    "\n",
    "# for user_folder in os.listdir(userlog_path):\n",
    "for folder_number in range(1, 13):\n",
    "    user_folder = f'P{folder_number}'\n",
    "    print(f'Processing {user_folder}')\n",
    "    user_data = {}\n",
    "    user_folder_path = os.path.join(userlog_path, user_folder)\n",
    "    if os.path.isdir(user_folder_path):\n",
    "        folder_number = int(user_folder[1:])\n",
    "        # if folder_number % 2 == 0:\n",
    "        for subdir, _, files in os.walk(user_folder_path):\n",
    "            for file in files:\n",
    "\n",
    "                str_rep = ''\n",
    "                \n",
    "                file_path = os.path.join(subdir, file)\n",
    "                if file == 'onboarding.json':\n",
    "                    pass\n",
    "                # if not file.lower().startswith('video1') and not file.lower().startswith('video2'): continue\n",
    "                elif file.lower().startswith('video1') or file.lower().startswith('video2'):\n",
    "                    video_data = {}\n",
    "                    vd_data = {}\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        bullet_points_data = json.load(f)\n",
    "                    video_data['pauseCount'] = bullet_points_data['pauseCount']\n",
    "                    video_data['forwardCount'] = bullet_points_data['forwardCount']\n",
    "                    video_data['reverseCount'] = bullet_points_data['reverseCount']\n",
    "                    video_data['point_count'] = len(bullet_points_data['editHistory'])\n",
    "                    video_data['expandCount'] = bullet_points_data['buttonStats']['expand_count']\n",
    "                    video_data['themeCount'] = bullet_points_data['buttonStats']['theme_count']\n",
    "                    summary_t = bullet_points_data['summary_t']\n",
    "                    summary_p = bullet_points_data['summary_p']\n",
    "                    note_points = [\n",
    "                        {\n",
    "                            'point': bpd['point'], \n",
    "                            'time_taken': bpd['note_taking_time'],\n",
    "                            'timestamp': bpd['utc_time'],\n",
    "                            'expanded_note': bpd['edit'][-1][0]['e_point'] if len(bpd['edit']) > 1 else None,\n",
    "                            'transcript': bpd['fraction_transcript'],\n",
    "                            'v_id': file.lower()\n",
    "                        }\n",
    "                        for bpd in bullet_points_data['editHistory']\n",
    "                    ]\n",
    "\n",
    "                    note_points = note_points[1:-1] # discarding first and last noisy point\n",
    "\n",
    "                    for bpd in bullet_points_data['editHistory']:\n",
    "                        str_rep += bpd['edit'][-1][0]['e_point']\n",
    "\n",
    "                    vd_data['p_id'] = user_folder\n",
    "                    vd_data['note_points'] = note_points\n",
    "                    vd_data['summary_p'] = summary_p\n",
    "                    vd_data['summary_t'] = summary_t\n",
    "                    if folder_number % 2 == 0:\n",
    "                        if file.lower().startswith('video1'): \n",
    "                            user_data['Baseline'] = video_data\n",
    "                            vd_data['micronote'] = False\n",
    "                            if 'video1' not in vd:\n",
    "                                vd['video1'] = []\n",
    "                            vd['video1'].append(vd_data)\n",
    "                            manual_note.append(str_rep)\n",
    "                        elif file.lower().startswith('video2'): \n",
    "                            user_data['NoTeeline'] = video_data\n",
    "                            vd_data['micronote'] = True\n",
    "                            if 'video2' not in vd:\n",
    "                                vd['video2'] = []\n",
    "                            vd['video2'].append(vd_data)\n",
    "                            auto_note.append(str_rep)\n",
    "                    else:\n",
    "                        if file.lower().startswith('video1'): \n",
    "                            user_data['NoTeeline'] = video_data\n",
    "                            vd_data['micronote'] = True\n",
    "                            if 'video1' not in vd:\n",
    "                                vd['video1'] = []\n",
    "                            vd['video1'].append(vd_data)\n",
    "                            auto_note.append(str_rep)\n",
    "                        elif file.lower().startswith('video2'): \n",
    "                            user_data['Baseline'] = video_data\n",
    "                            vd_data['micronote'] = False\n",
    "                            if 'video2' not in vd:\n",
    "                                vd['video2'] = []\n",
    "                            vd['video2'].append(vd_data)\n",
    "                            manual_note.append(str_rep)\n",
    "    data.append(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_notee = [None]*12\n",
    "with open(f'./UserLog/Expansions/UNIQUE_TOPK/user11_no_seed_0.3_temp_1106_model.txt', 'r') as file:\n",
    "    author_notee[10] = file.read()\n",
    "\n",
    "for i in range(12):\n",
    "    if i != 10:\n",
    "        with open(f'./UserLog/Expansions/UNIQUE_TOPK/user{i+1}_no_seed_0.3_temp_1106_model.txt', 'r') as file:\n",
    "            author_notee[i] = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "federalist_by_author_tokens = {}\n",
    "federalist_by_author_length_distributions = {}\n",
    "federalist_by_author_tokens_baseline = {}\n",
    "federalist_by_author_length_distributions_baseline = {}\n",
    "federalist_by_author_tokens_special = {}\n",
    "federalist_by_author_length_distributions_special = {}\n",
    "\n",
    "for author in range(12):\n",
    "    tokens = nltk.word_tokenize(auto_note[author])\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "    federalist_by_author_tokens[author] = tokens\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens[author]]\n",
    "    federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "\n",
    "    #----------------without onboarding----------------\n",
    "\n",
    "    tokens = nltk.word_tokenize(author_notee[author])\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "    federalist_by_author_tokens_special[author] = tokens\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens_special[author]]\n",
    "    federalist_by_author_length_distributions_special[author] = nltk.FreqDist(token_lengths)\n",
    "\n",
    "\n",
    "    # ------------Baseline--------------\n",
    "\n",
    "    tokens = nltk.word_tokenize(manual_note[author])\n",
    "    tokens = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "    federalist_by_author_tokens_baseline[author] = tokens\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens_baseline[author]]\n",
    "    federalist_by_author_length_distributions_baseline[author] = nltk.FreqDist(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in range(12):\n",
    "    federalist_by_author_tokens[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens[author]])\n",
    "\n",
    "    federalist_by_author_tokens_special[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens_special[author]])\n",
    "\n",
    "    federalist_by_author_tokens_baseline[author] = (\n",
    "        [token.lower() for token in federalist_by_author_tokens_baseline[author]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_with_onboarding = []\n",
    "chi_without_onboarding = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noteeline - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chi-squared statistic for candidate 1 is 449.10728175145294\n",
      "The Chi-squared statistic for candidate 2 is 335.2114282548128\n",
      "The Chi-squared statistic for candidate 3 is 342.70897626664333\n",
      "The Chi-squared statistic for candidate 4 is 297.95545314900113\n",
      "The Chi-squared statistic for candidate 5 is 279.94304370931485\n",
      "The Chi-squared statistic for candidate 6 is 423.8414404694718\n",
      "The Chi-squared statistic for candidate 7 is 279.57640794739916\n",
      "The Chi-squared statistic for candidate 8 is 417.7428552572537\n",
      "The Chi-squared statistic for candidate 9 is 376.39782045654346\n",
      "The Chi-squared statistic for candidate 10 is 287.53174095725353\n",
      "The Chi-squared statistic for candidate 11 is 224.37544063079798\n",
      "The Chi-squared statistic for candidate 12 is 496.3760338112809\n"
     ]
    }
   ],
   "source": [
    "for author in range(12):\n",
    "    # Calculate chisquared for each of the two candidate authors\n",
    "\n",
    "    sum = 0\n",
    "\n",
    "    # First, build a joint corpus and identify the 500 most frequent words in it\n",
    "    joint_corpus = (federalist_by_author_tokens[author] +\n",
    "                    federalist_by_author_tokens_baseline[author])\n",
    "    joint_freq_dist = nltk.FreqDist(joint_corpus)\n",
    "    most_common = list(joint_freq_dist.most_common(500))\n",
    "\n",
    "    # What proportion of the joint corpus is made up\n",
    "    # of the candidate author's tokens?\n",
    "    author_share = (len(federalist_by_author_tokens[author])\n",
    "                    / len(joint_corpus))\n",
    "\n",
    "    # Now, let's look at the 500 most common words in the candidate\n",
    "    # author's corpus and compare the number of times they can be observed\n",
    "    # to what would be expected if the author's papers\n",
    "    # and the Disputed papers were both random samples from the same distribution.\n",
    "    chisquared = 0\n",
    "    for word,joint_count in most_common:\n",
    "\n",
    "        # How often do we really see this common word?\n",
    "        author_count = federalist_by_author_tokens[author].count(word)\n",
    "        disputed_count = federalist_by_author_tokens_baseline[author].count(word)\n",
    "\n",
    "        # How often should we see it?\n",
    "        expected_author_count = joint_count * author_share\n",
    "        expected_disputed_count = joint_count * (1-author_share)\n",
    "\n",
    "        # Add the word's contribution to the chi-squared statistic\n",
    "        chisquared += ((author_count-expected_author_count) *\n",
    "                        (author_count-expected_author_count) /\n",
    "                        expected_author_count)\n",
    "\n",
    "        chisquared += ((disputed_count-expected_disputed_count) *\n",
    "                        (disputed_count-expected_disputed_count)\n",
    "                        / expected_disputed_count)\n",
    "\n",
    "    print(\"The Chi-squared statistic for candidate\", author+1, \"is\", chisquared)\n",
    "    chi_with_onboarding.append(chisquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noteeline(without onboarding) - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chi-squared statistic for candidate 1 is 423.7648326659038\n",
      "The Chi-squared statistic for candidate 2 is 375.4741324280407\n",
      "The Chi-squared statistic for candidate 3 is 346.8186275676078\n",
      "The Chi-squared statistic for candidate 4 is 336.85593927893456\n",
      "The Chi-squared statistic for candidate 5 is 312.99848752631397\n",
      "The Chi-squared statistic for candidate 6 is 428.97715544198496\n",
      "The Chi-squared statistic for candidate 7 is 309.28261049874544\n",
      "The Chi-squared statistic for candidate 8 is 402.6442460554406\n",
      "The Chi-squared statistic for candidate 9 is 480.90782622488945\n",
      "The Chi-squared statistic for candidate 10 is 353.18386603153766\n",
      "The Chi-squared statistic for candidate 11 is 242.52462133744487\n",
      "The Chi-squared statistic for candidate 12 is 579.9258657977979\n"
     ]
    }
   ],
   "source": [
    "for author in range(12):\n",
    "    # Calculate chisquared for each of the two candidate authors\n",
    "\n",
    "    sum = 0\n",
    "\n",
    "    # First, build a joint corpus and identify the 500 most frequent words in it\n",
    "    joint_corpus = (federalist_by_author_tokens_special[author] +\n",
    "                    federalist_by_author_tokens_baseline[author])\n",
    "    joint_freq_dist = nltk.FreqDist(joint_corpus)\n",
    "    most_common = list(joint_freq_dist.most_common(500))\n",
    "\n",
    "    # What proportion of the joint corpus is made up\n",
    "    # of the candidate author's tokens?\n",
    "    author_share = (len(federalist_by_author_tokens_special[author])\n",
    "                    / len(joint_corpus))\n",
    "\n",
    "    # Now, let's look at the 500 most common words in the candidate\n",
    "    # author's corpus and compare the number of times they can be observed\n",
    "    # to what would be expected if the author's papers\n",
    "    # and the Disputed papers were both random samples from the same distribution.\n",
    "    chisquared = 0\n",
    "    for word,joint_count in most_common:\n",
    "\n",
    "        # How often do we really see this common word?\n",
    "        author_count = federalist_by_author_tokens_special[author].count(word)\n",
    "        disputed_count = federalist_by_author_tokens_baseline[author].count(word)\n",
    "\n",
    "        # How often should we see it?\n",
    "        expected_author_count = joint_count * author_share\n",
    "        expected_disputed_count = joint_count * (1-author_share)\n",
    "\n",
    "        # Add the word's contribution to the chi-squared statistic\n",
    "        chisquared += ((author_count-expected_author_count) *\n",
    "                        (author_count-expected_author_count) /\n",
    "                        expected_author_count)\n",
    "\n",
    "        chisquared += ((disputed_count-expected_disputed_count) *\n",
    "                        (disputed_count-expected_disputed_count)\n",
    "                        / expected_disputed_count)\n",
    "\n",
    "    print(\"The Chi-squared statistic for candidate\", author+1, \"is\", chisquared)\n",
    "    chi_without_onboarding.append(chisquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      NoTeeline    Baseline\n",
      "P1   449.107282  423.764833\n",
      "P2   335.211428  375.474132\n",
      "P3   342.708976  346.818628\n",
      "P4   297.955453  336.855939\n",
      "P5   279.943044  312.998488\n",
      "P6   423.841440  428.977155\n",
      "P7   279.576408  309.282610\n",
      "P8   417.742855  402.644246\n",
      "P9   376.397820  480.907826\n",
      "P10  287.531741  353.183866\n",
      "P11  224.375441  242.524621\n",
      "P12  496.376034  579.925866\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'NoTeeline': chi_with_onboarding,\n",
    "    'Baseline': chi_without_onboarding\n",
    "})\n",
    "\n",
    "df.index = ['P' + str(i) for i in range(1, len(df) + 1)]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
